{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineering Bootcamp Cohort 4 Midterm\n",
    "\n",
    "For details of this project please see the project Readme.\n",
    "\n",
    "This notebooks takes advantage of shared state classes and some common utility functions shared with the Chainlit application and other notebooks\n",
    "\n",
    "The state classes are:\n",
    "- AppState - contains information about the application and the documents we are looking at\n",
    "- ModelRunState - contains information for each individual run for a model, chunking parameters, and results \n",
    "- RagasState - contains information for Ragas evaluation including the questions and context\n",
    "\n",
    "The state is initialized and is passed between the functions. Each model/chunking strategy/parameters can run and save their information and then can be easily compared. \n",
    "\n",
    "This utilities include:\n",
    "- constants.py - constants used throughout the package (mainly for the chunking strategies)\n",
    "- debugger.py - supports printing messages for when debug=True\n",
    "- doc_utilities.py - supports reading the documents\n",
    "- rag_utilities.py - supports the creation of the RAG chain \n",
    "- templates.py - provides templates for RAG chains\n",
    "- vector_utilities - provides functions for chunking documents and setting up the vector store. Includes 4 chunking strategies:\n",
    "    - Recursive splitter with chunk size and overlap\n",
    "    - Table aware - tries to handle pdfs with tables\n",
    "    - Section based - chunks based on section headers\n",
    "    - Semantic splitter - semantic-based chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow for async in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps hide errors from using Hugging Face's transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install our key components for RAG etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q langchain-core==0.2.27 langchain-community==0.2.10\n",
    "!pip install -q langchain-experimental==0.0.64 langgraph-checkpoint==1.0.6 langgraph==0.2.16 langchain-qdrant==0.1.3\n",
    "!pip install -q langchain-openai==0.1.9\n",
    "!pip install -q ragas==0.1.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install our vector store - Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU qdrant-client==1.11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install supporting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU tiktoken==0.7.0 pymupdf==1.24.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Variables\n",
    "\n",
    "- OpenAI API Key - will use some of the OpenAI models - if in .env use it otherwise ask for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#if not openai_api_key:\n",
    "openai_api_key = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our starting inputs and state and read in the documents\n",
    "\n",
    "The AppState contains the docs to be used throughout the process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 2\n"
     ]
    }
   ],
   "source": [
    "from classes.app_state import AppState\n",
    "from utilities.doc_utilities import get_documents\n",
    "document_urls = [\n",
    "    \"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\",\n",
    "    \"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\",\n",
    "]\n",
    "\n",
    "app_state = AppState()\n",
    "app_state.set_debug(False)\n",
    "\n",
    "app_state.set_document_urls(document_urls)\n",
    "\n",
    "get_documents(app_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store with text-embedding-3-small embeddings\n",
    "\n",
    "Set up our first model run - using the text-embedding-3-small for embeddings initially\n",
    "\n",
    "This will test that our chunking and vector store functions are all working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n"
     ]
    }
   ],
   "source": [
    "from classes.model_run_state import ModelRunState\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from utilities.vector_utilities import create_vector_store\n",
    "\n",
    "model_1000_100_state = ModelRunState()\n",
    "model_1000_100_state.name = \"TE3/1000/100\"\n",
    "model_1000_100_state.chunk_size = 1000\n",
    "model_1000_100_state.chunk_overlap = 100\n",
    "\n",
    "model_1000_100_state.qa_model_name = \"gpt-4o-mini\"\n",
    "model_1000_100_state.qa_model = ChatOpenAI(model=model_1000_100_state.qa_model_name)\n",
    "\n",
    "# the openai embedding model\n",
    "model_1000_100_state.embedding_model_name = \"text-embedding-3-small\"\n",
    "model_1000_100_state.embedding_model = OpenAIEmbeddings(model=model_1000_100_state.embedding_model_name)\n",
    "\n",
    "create_vector_store(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the retriever with some sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "You should be protected from abusive data practices via built-in \n",
      "protections and you should have agency over how data about \n",
      "you is used. You should be protected from violations of privacy through \n",
      "design choices that ensure such protections are included by default, including \n",
      "ensuring that data collection conforms to reasonable expectations and that \n",
      "only data strictly necessary for the specific context is collected. Designers, de­\n",
      "velopers, and deployers of automated systems should seek your permission \n",
      "and respect your decisions regarding collection, use, access, transfer, and de­\n",
      "letion of your data in appropriate ways and to the greatest extent possible; \n",
      "where not possible, alternative privacy by design safeguards should be used. \n",
      "Systems should not employ user experience and design decisions that obfus­\n",
      "cate user choice or burden users with defaults that are privacy invasive. Con­\n",
      "sent should only be used to justify collection of data in cases where it can be \n",
      "appropriately and meaningfully given. Any consent requests should be brief, \n",
      "be understandable in plain language, and give you agency over data collection \n",
      "and the specific context of use; current hard-to-understand no­\n",
      "tice-and-choice practices for broad uses of data should be changed. Enhanced \n",
      "protections and restrictions for data and inferences related to sensitive do­\n",
      "mains, including health, work, education, criminal justice, and finance, and \n",
      "for data pertaining to youth should put you first. In sensitive domains, your \n",
      "data and related inferences should only be used for necessary functions, and \n",
      "you should be protected by ethical review and use prohibitions. You and your \n",
      "communities should be free from unchecked surveillance; surveillance tech­\n",
      "nologies should be subject to heightened oversight that includes at least \n",
      "pre-deployment assessment of their potential harms and scope limits to pro­\n",
      "tect privacy and civil liberties. Continuous surveillance and monitoring \n",
      "should not be used in education, work, housing, or in other contexts where the \n",
      "use of such surveillance technologies is likely to limit rights, opportunities, or \n",
      "access. Whenever possible, you should have access to reporting that confirms \n",
      "your data decisions have been respected and provides an assessment of the \n",
      "potential impact of surveillance technologies on your rights, opportunities, or \n",
      "access. \n",
      "DATA PRIVACY\n",
      "30\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 25, '_id': '910b1a9223f14116b58de0dd805d85d8', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How should you be protected from abusive data practices \"\n",
    "results = model_1000_100_state.retriever.get_relevant_documents(query)\n",
    "\n",
    "print(len(results))\n",
    "print(results[0].page_content)\n",
    "print(results[0].metadata)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDNOTES\n",
      "75. See., e.g., Sam Sabin. Digital surveillance in a post-Roe world. Politico. May 5, 2022. https://\n",
      "www.politico.com/newsletters/digital-future-daily/2022/05/05/digital-surveillance-in-a-post-roe­\n",
      "world-00030459; Federal Trade Commission. FTC Sues Kochava for Selling Data that Tracks People at\n",
      "Reproductive Health Clinics, Places of Worship, and Other Sensitive Locations. Aug. 29, 2022. https://\n",
      "www.ftc.gov/news-events/news/press-releases/2022/08/ftc-sues-kochava-selling-data-tracks-people­\n",
      "reproductive-health-clinics-places-worship-other\n",
      "76. Todd Feathers. This Private Equity Firm Is Amassing Companies That Collect Data on America’s\n",
      "Children. The Markup. Jan. 11, 2022.\n",
      "https://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies­\n",
      "that-collect-data-on-americas-children\n",
      "77. Reed Albergotti. Every employee who leaves Apple becomes an ‘associate’: In job databases used by\n",
      "employers to verify resume information, every former Apple employee’s title gets erased and replaced with\n",
      "a generic title. The Washington Post. Feb. 10, 2022.\n",
      "https://www.washingtonpost.com/technology/2022/02/10/apple-associate/\n",
      "78. National Institute of Standards and Technology. Privacy Framework Perspectives and Success\n",
      "Stories. Accessed May 2, 2022.\n",
      "https://www.nist.gov/privacy-framework/getting-started-0/perspectives-and-success-stories\n",
      "79. ACLU of New York. What You Need to Know About New York’s Temporary Ban on Facial\n",
      "Recognition in Schools. Accessed May 2, 2022.\n",
      "https://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial­\n",
      "recognition-schools\n",
      "80. New York State Assembly. Amendment to Education Law. Enacted Dec. 22, 2020.\n",
      "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=S05140&term=2019&Summary=Y&Text=Y\n",
      "81. U.S Department of Labor. Labor-Management Reporting and Disclosure Act of 1959, As Amended.\n",
      "https://www.dol.gov/agencies/olms/laws/labor-management-reporting-and-disclosure-act (Section\n",
      "203). See also: U.S Department of Labor. Form LM-10. OLMS Fact Sheet, Accessed May 2, 2022. https://\n",
      "www.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf\n",
      "82. See, e.g., Apple. Protecting the User’s Privacy. Accessed May 2, 2022.\n",
      "https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.\n",
      "Design for Safety: Android is secure by default and private by design. Accessed May 3, 2022.\n",
      "https://developer.android.com/design-for-safety\n",
      "83. Karen Hao. The coming war on the hidden algorithms that trap people in poverty. MIT Tech Review.\n",
      "Dec. 4, 2020.\n",
      "https://www.technologyreview.com/2020/12/04/1013068/algorithms-create-a-poverty-trap-lawyers­\n",
      "fight-back/\n",
      "84. Anjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers. Family Surveillance by Algorithm.\n",
      "ACLU. Accessed May 2, 2022.\n",
      "https://www.aclu.org/fact-sheet/family-surveillance-algorithm\n",
      "70\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 58, '_id': '2323da572c654e81adaa1c8b4f54d0e0', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "ENDNOTES\n",
      "57. ISO Technical Management Board. ISO/IEC Guide 71:2014. Guide for addressing accessibility in\n",
      "standards. International Standards Organization. 2021. https://www.iso.org/standard/57385.html\n",
      "58. World Wide Web Consortium. Web Content Accessibility Guidelines (WCAG) 2.0. Dec. 11, 2008.\n",
      "https://www.w3.org/TR/WCAG20/\n",
      "59. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert. NIST Special\n",
      "Publication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. The\n",
      "National Institute of Standards and Technology. March, 2022. https://nvlpubs.nist.gov/nistpubs/\n",
      "SpecialPublications/NIST.SP.1270.pdf\n",
      "60. See, e.g., the 2014 Federal Trade Commission report “Data Brokers A Call for Transparency and\n",
      "Accountability”. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency­\n",
      "accountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf\n",
      "61. See, e.g., Nir Kshetri. School surveillance of students via laptops may do more harm than good. The\n",
      "Conversation. Jan. 21, 2022.\n",
      "https://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than­\n",
      "good-170983; Matt Scherer. Warning: Bossware May be Hazardous to Your Health. Center for Democracy\n",
      "& Technology Report.\n",
      "https://cdt.org/wp-content/uploads/2021/07/2021-07-29-Warning-Bossware-May-Be-Hazardous-To­\n",
      "Your-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon\n",
      "Warehouses. HIP and WWRC report. Jan. 2021.\n",
      "https://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon­\n",
      "Warehouses-HIP-WWRC-01-21.pdf; Drew Harwell. Contract lawyers face a growing invasion of\n",
      "surveillance programs that monitor their work. The Washington Post. Nov. 11, 2021. https://\n",
      "www.washingtonpost.com/technology/2021/11/11/lawyer-facial-recognition-monitoring/;\n",
      "Virginia Doellgast and Sean O'Brady. Making Call Center Jobs Better: The Relationship between\n",
      "Management Practices and Worker Stress. A Report for the CWA. June 2020. https://\n",
      "hdl.handle.net/1813/74307\n",
      "62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May\n",
      "2014.\n",
      "https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability­\n",
      "report-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O’Neil.\n",
      "Weapons of Math Destruction. Penguin Books. 2017.\n",
      "https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction\n",
      "63. See, e.g., Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel. Social Media Surveillance by\n",
      "the U.S. Government. Brennan Center for Justice. Jan. 7, 2022.\n",
      "https://www.brennancenter.org/our-work/research-reports/social-media-surveillance-us-government;\n",
      "Shoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of\n",
      "Power. Public Affairs. 2019.\n",
      "64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.\n",
      "7, 2019.\n",
      "https://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big­\n",
      "data-discrimination-online-records\n",
      "68\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 56, '_id': 'f97b4fea3aa04175ab9161b904041a40', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "60 \n",
      "Zhang, Y. et al. (2023) Human favoritism, not AI aversion: People’s perceptions (and bias) toward \n",
      "generative AI, human experts, and human–GAI collaboration in persuasive content generation. Judgment \n",
      "and Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\n",
      "making/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\n",
      "human-experts-and-humangai-collaboration-in-persuasive-content-\n",
      "generation/419C4BD9CE82673EAF1D8F6C350C4FA8 \n",
      "Zhang, Y. et al. (2023) Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \n",
      "arXiv. https://arxiv.org/pdf/2309.01219 \n",
      "Zhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \n",
      "https://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\n",
      "Ananth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc\n",
      "{'source': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'document_id': '8309b4cf-ef7d-4435-b580-65e8f86d34fd', 'chunk_number': 53, '_id': 'a1268c0cdfb848b6b53e4312214e50b8', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "APPENDIX\n",
      "Panel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \n",
      "technology that impact equity of opportunity in employment, education, and housing. \n",
      "Welcome: \n",
      "•\n",
      "Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "Dominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic\n",
      "Studies\n",
      "Moderator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \n",
      "Panelists: \n",
      "•\n",
      "Christo Wilson, Associate Professor of Computer Science, Northeastern University\n",
      "•\n",
      "Frida Polli, CEO, Pymetrics\n",
      "•\n",
      "Karen Levy, Assistant Professor, Department of Information Science, Cornell University\n",
      "•\n",
      "Natasha Duarte, Project Director, Upturn\n",
      "•\n",
      "Elana Zeide, Assistant Professor, University of Nebraska College of Law\n",
      "•\n",
      "Fabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\n",
      "Advocate and Floor Captain, Atlantic Plaza Towers Tenants Association\n",
      "The individual panelists described the ways in which AI systems and other technologies are increasingly being \n",
      "used to limit access to equal opportunities in education, housing, and employment. Education-related \n",
      "concerning uses included the increased use of remote proctoring systems, student location and facial \n",
      "recognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses \n",
      "including automated tenant background screening and facial recognition-based controls to enter or exit \n",
      "housing complexes. Employment-related concerning uses included discrimination in automated hiring \n",
      "screening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \n",
      "concern, pointing out that students should be able to reinvent themselves and require privacy of their student \n",
      "records and education-related data in order to do so. The overarching concerns of surveillance in these \n",
      "domains included concerns about the chilling effects of surveillance on student expression, inappropriate \n",
      "control of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work \n",
      "and life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \n",
      "pointed out ways that data from one situation was misapplied in another in a way that limited people's \n",
      "opportunities, for example data from criminal justice settings or previous evictions being used to block further \n",
      "access to housing. Throughout, various panelists emphasized that these technologies are being used to shift the \n",
      "burden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in \n",
      "ways that diminish and encroach on equality of opportunity; assessment of these technologies should include \n",
      "whether they are genuinely helpful in solving an identified problem. \n",
      "In discussion of technical and governance interventions that that are needed to protect against the harms of \n",
      "these technologies, panelists individually described the importance of: receiving community input into the \n",
      "design and use of technologies, public reporting on crucial elements of these systems, better notice and consent \n",
      "procedures that ensure privacy based on context and use case, ability to opt-out of using these systems and \n",
      "receive a fallback to a human process, providing explanations of decisions and how these systems work, the \n",
      "need for governance including training in using these systems, ensuring the technological use cases are \n",
      "genuinely related to the goal task and are locally validated to work, and the need for institution and protection \n",
      "of third party audits to ensure systems continue to be accountable and valid. \n",
      "57\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 46, '_id': 'a7e83942470648df90b54e84a7d83df8', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "SECTION TITLE\n",
      "APPENDIX\n",
      "Listening to the American People \n",
      "The White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill \n",
      "input from people across the country – from impacted communities to industry stakeholders to \n",
      "technology developers to other experts across fields and sectors, as well as policymakers across the Federal \n",
      "government – on the issue of algorithmic and data-driven harms and potential remedies. Through panel \n",
      "discussions, public listening sessions, private meetings, a formal request for information, and input to a \n",
      "publicly accessible and widely-publicized email address, people across the United States spoke up about \n",
      "both the promises and potential harms of these technologies, and played a central role in shaping the \n",
      "Blueprint for an AI Bill of Rights. \n",
      "Panel Discussions to Inform the Blueprint for An AI Bill of Rights \n",
      "OSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress, \n",
      "the Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic \n",
      "Privacy Information Center, and the Mozilla Foundation. The purpose of these convenings – recordings of \n",
      "which are publicly available online112 – was to bring together a variety of experts, practitioners, advocates \n",
      "and federal government officials to offer insights and analysis on the risks, harms, benefits, and \n",
      "policy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \n",
      "theme, exploring current challenges and concerns and considering what an automated society that \n",
      "respects democratic values should look like. These discussions focused on the topics of consumer \n",
      "rights and protections, the criminal justice system, equal opportunities and civil justice, artificial \n",
      "intelligence and democratic values, social welfare and development, and the healthcare system. \n",
      "Summaries of Panel Discussions: \n",
      "Panel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for \n",
      "individual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \n",
      "products, advanced platforms and services, “Internet of Things” (IoT) devices, and smart city products and \n",
      "services. \n",
      "Welcome:\n",
      "•\n",
      "Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "Karen Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German\n",
      "Marshall Fund\n",
      "Moderator: \n",
      "Devin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal \n",
      "Trade Commission \n",
      "Panelists: \n",
      "•\n",
      "Tamika L. Butler, Principal, Tamika L. Butler Consulting\n",
      "•\n",
      "Jennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\n",
      "State University\n",
      "•\n",
      "Carl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\n",
      "•\n",
      "Surya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\n",
      "•\n",
      "Mariah Montgomery, National Campaign Director, Partnership for Working Families\n",
      "55\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 44, '_id': '5443150c9c3b4fcc96b6d967e48d28e1', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "APPENDIX\n",
      "Panel 4: Artificial Intelligence and Democratic Values. This event examined challenges and opportunities in \n",
      "the design of technology that can help support a democratic vision for AI. It included discussion of the \n",
      "technical aspects \n",
      "of \n",
      "designing \n",
      "non-discriminatory \n",
      "technology, \n",
      "explainable \n",
      "AI, \n",
      "human-computer \n",
      "interaction with an emphasis on community participation, and privacy-aware design. \n",
      "Welcome:\n",
      "•\n",
      "Sorelle Friedler, Assistant Director for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "J. Bob Alotta, Vice President for Global Programs, Mozilla Foundation\n",
      "•\n",
      "Navrina Singh, Board Member, Mozilla Foundation\n",
      "Moderator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S \n",
      "Federal Trade Commission. \n",
      "Panelists: \n",
      "•\n",
      "Liz O’Sullivan, CEO, Parity AI\n",
      "•\n",
      "Timnit Gebru, Independent Scholar\n",
      "•\n",
      "Jennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City\n",
      "•\n",
      "Pamela Wisniewski, Associate Professor of Computer Science, University of Central Florida; Director,\n",
      "Socio-technical Interaction Research (STIR) Lab\n",
      "•\n",
      "Seny Kamara, Associate Professor of Computer Science, Brown University\n",
      "Each panelist individually emphasized the risks of using AI in high-stakes settings, including the potential for \n",
      "biased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and \n",
      "understanding of the algorithmic systems. The interventions and key needs various panelists put forward as \n",
      "necessary to the future design of critical AI systems included ongoing transparency, value sensitive and \n",
      "participatory design, explanations designed for relevant stakeholders, and public consultation. \n",
      "Various \n",
      "panelists emphasized the importance of placing trust in people, not technologies, and in engaging with \n",
      "impacted communities to understand the potential harms of technologies and build protection by design into \n",
      "future systems. \n",
      "Panel 5: Social Welfare and Development. This event explored current and emerging uses of technology to \n",
      "implement or improve social welfare systems, social development programs, and other systems that can impact \n",
      "life chances. \n",
      "Welcome:\n",
      "•\n",
      "Suresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\n",
      "and Technology Policy\n",
      "•\n",
      "Anne-Marie Slaughter, CEO, New America\n",
      "Moderator: Michele Evermore, Deputy Director for Policy, Office of Unemployment Insurance \n",
      "Modernization, Office of the Secretary, Department of Labor \n",
      "Panelists:\n",
      "•\n",
      "Blake Hall, CEO and Founder, ID.Me\n",
      "•\n",
      "Karrie Karahalios, Professor of Computer Science, University of Illinois, Urbana-Champaign\n",
      "•\n",
      "Christiaan van Veen, Director of Digital Welfare State and Human Rights Project, NYU School of Law's\n",
      "Center for Human Rights and Global Justice\n",
      "58\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 47, '_id': '4f9615b1d8f64e0286b720460b7a6f42', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "59 \n",
      "Tirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n",
      "139-162. https://www.jstor.org/stable/26529441  \n",
      "Tufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \n",
      "Computational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\n",
      "content/uploads/2015/08/Tufekci-ﬁnal.pdf \n",
      "Turri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \n",
      "Practices. AAAI/ACM Conference on AI, Ethics, and Society. \n",
      "https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \n",
      "Urbina, F. et al. (2022) Dual use of artiﬁcial-intelligence-powered drug discovery. Nature Machine \n",
      "Intelligence. https://www.nature.com/articles/s42256-022-00465-9 \n",
      "Wang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \n",
      "https://aclanthology.org/2023.ﬁndings-emnlp.607.pdf \n",
      "Wang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \n",
      "https://arxiv.org/pdf/2308.13387 \n",
      "Wardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \n",
      "policy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\n",
      "framework-for-researc/168076277c \n",
      "Weatherbed, J. (2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge. \n",
      "https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \n",
      "Wei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \n",
      "https://arxiv.org/pdf/2403.18802 \n",
      "Weidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \n",
      "https://arxiv.org/pdf/2112.04359 \n",
      "Weidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \n",
      "https://arxiv.org/pdf/2310.11986 \n",
      "Weidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \n",
      "https://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \n",
      "West, D. (2023) AI poses disproportionate risks to women. Brookings. \n",
      "https://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \n",
      "Wu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \n",
      "analyses. arXiv. https://arxiv.org/pdf/2402.02008 \n",
      "Yin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \n",
      "https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \n",
      "Yu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \n",
      "Language Models. arXiv. https://arxiv.org/html/2403.17336v1 \n",
      "Zaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \n",
      "https://policyreview.info/pdf/policyreview-2022-2-1654.pdf\n",
      "{'source': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'document_id': '8309b4cf-ef7d-4435-b580-65e8f86d34fd', 'chunk_number': 52, '_id': '8a00d6f787ea4df2bde19aca36255b21', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "ENDNOTES\n",
      "35. Carrie Johnson. Flaws plague a tool meant to help low-risk federal prisoners win early release. NPR.\n",
      "Jan. 26, 2022. https://www.npr.org/2022/01/26/1075509175/flaws-plague-a-tool-meant-to-help-low­\n",
      "risk-federal-prisoners-win-early-release.; Carrie Johnson. Justice Department works to curb racial bias\n",
      "in deciding who's released from prison. NPR. Apr. 19, 2022. https://\n",
      "www.npr.org/2022/04/19/1093538706/justice-department-works-to-curb-racial-bias-in-deciding­\n",
      "whos-released-from-pris; National Institute of Justice. 2021 Review and Revalidation of the First Step Act\n",
      "Risk Assessment Tool. National Institute of Justice NCJ 303859. Dec., 2021. https://www.ojp.gov/\n",
      "pdffiles1/nij/303859.pdf\n",
      "36. Andrew Thompson. Google’s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://\n",
      "www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias\n",
      "37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of\n",
      "conversations. 2019. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n",
      "38. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and\n",
      "Mitigating Unintended Bias in Text Classification. Proceedings of AAAI/ACM Conference on AI, Ethics,\n",
      "and Society. Feb. 2-3, 2018. https://dl.acm.org/doi/pdf/10.1145/3278721.3278729\n",
      "39. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n",
      "2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina­\n",
      "teenager-2022-03-30/\n",
      "40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.\n",
      "Feb. 2018. https://nyupress.org/9781479837243/algorithms-of-oppression/\n",
      "41. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n",
      "2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina­\n",
      "teenager-2022-03-30/\n",
      "42. Miranda Bogen. All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. May\n",
      "6, 2019. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias\n",
      "43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil\n",
      "Liberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making­\n",
      "flying-easier-for-transgender-people\n",
      "44. U.S. Transportation Security Administration. Transgender/ Non Binary / Gender Nonconforming\n",
      "Passengers. TSA. Accessed Apr. 21, 2022. https://www.tsa.gov/transgender-passengers\n",
      "45. See, e.g., National Disabled Law Students Association. Report on Concerns Regarding Online\n",
      "Administration of Bar Exams. Jul. 29, 2020. https://ndlsa.org/wp-content/uploads/2020/08/\n",
      "NDLSA_Online-Exam-Concerns-Report1.pdf; Lydia X. Z. Brown. How Automated Test Proctoring\n",
      "Software Discriminates Against Disabled Students. Center for Democracy and Technology. Nov. 16, 2020.\n",
      "https://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled­\n",
      "students/\n",
      "46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of\n",
      "populations, 366 Science (2019), https://www.science.org/doi/10.1126/science.aax2342.\n",
      "66\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 54, '_id': 'd882bed1817e441281fd913daaf2780c', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\n",
      "Appears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\n",
      "www.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles­\n",
      "in-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,\n",
      "Nov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\n",
      "66. Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.\n",
      "Sept. 24, 2019.\n",
      "https://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\n",
      "67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust\n",
      "Unions. Newsweek. Dec. 13, 2021.\n",
      "https://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust­\n",
      "unions-1658603\n",
      "68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\n",
      "(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\n",
      "against Weight Watchers and their subsidiary Kurbo\n",
      "(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\n",
      "69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\n",
      "(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online\n",
      "Privacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection and\n",
      "Statistical Efficiency Act (CIPSEA) (116 Stat. 2899)\n",
      "70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\n",
      "21, 2018.\n",
      "https://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\n",
      "71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\n",
      "https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n",
      "72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\n",
      "Schools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\n",
      "https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology­\n",
      "schools-are-using-to-monitor-students/\n",
      "73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\n",
      "fighting back. Washington Post. Nov. 12, 2020.\n",
      "https://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\n",
      "74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\n",
      "Technology. May 24, 2022.\n",
      "https://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\n",
      "Lydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\n",
      "Discrimination In New Surveillance Technologies: How new surveillance technologies in education,\n",
      "policing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\n",
      "and Technology Report. May 24, 2022.\n",
      "https://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how­\n",
      "new-surveillance-technologies-in-education-policing-health-care-and-the-workplace­\n",
      "disproportionately-harm-disabled-people/\n",
      "69\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 57, '_id': 'a996059fd2cc4f4f8bbd99a817c7d9fc', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n",
      "ENDNOTES\n",
      "1.The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the\n",
      "Federal Government. https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive\n",
      "order-advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\n",
      "2. The White House. Remarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade. Jun.\n",
      "24, 2022. https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/06/24/remarks-by-president­\n",
      "biden-on-the-supreme-court-decision-to-overturn-roe-v-wade/\n",
      "3. The White House. Join the Effort to Create A Bill of Rights for an Automated Society. Nov. 10, 2021. https://\n",
      "www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an­\n",
      "automated-society/\n",
      "4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec’y’s Advisory Comm. on Automated Pers. Data Sys.,\n",
      "Records, Computers, and the Rights of Citizens (July 1973). https://www.justice.gov/opcl/docs/rec-com­\n",
      "rights.pdf.\n",
      "5. See, e.g., Office of Mgmt. & Budget, Exec. Office of the President, Circular A-130, Managing Information as a\n",
      "Strategic Resource, app. II § 3 (July 28, 2016); Org. of Econ. Co-Operation & Dev., Revision of the\n",
      "Recommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder\n",
      "Flows of Personal Data, Annex Part Two (June 20, 2013). https://one.oecd.org/document/C(2013)79/en/pdf.\n",
      "6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in\n",
      "hospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626\n",
      "7. Jessica Guynn. Facebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate\n",
      "speech. USA Today. Apr. 24, 2019. https://www.usatoday.com/story/news/2019/04/24/facebook-while-black­\n",
      "zucked-users-say-they-get-blocked-racism-discussion/2859593002/\n",
      "8. See, e.g., Michael Levitt. AirTags are being used to track people and cars. Here's what is being done about it.\n",
      "NPR. Feb. 18, 2022. https://www.npr.org/2022/02/18/1080944193/apple-airtags-theft-stalking-privacy-tech;\n",
      "Samantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.\n",
      "Motherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking­\n",
      "harassment\n",
      "9. Kristian Lum and William Isaac. To Predict and Serve? Significance. Vol. 13, No. 5, p. 14-19. Oct. 7, 2016.\n",
      "https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Aaron Sankin, Dhruv Mehrotra,\n",
      "Surya Mattu, and Annie Gilbertson. Crime Prediction Software Promised to Be Free of Biases. New Data Shows\n",
      "It Perpetuates Them. The Markup and Gizmodo. Dec. 2, 2021. https://themarkup.org/prediction­\n",
      "bias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates­\n",
      "them\n",
      "10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.\n",
      "June 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman\n",
      "11. Lauren Kaori Gurley. Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make.\n",
      "Motherboard. Sep. 20, 2021. https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing­\n",
      "drivers-for-mistakes-they-didnt-make\n",
      "63\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': '041b240b-0921-41e7-adc2-e64abebf0255', 'chunk_number': 51, '_id': '44d58770eab344418ddb1a8ec2f0bfb8', '_collection_name': '361c20f6d6a745fb98bb2d5f8950428f'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me about Karen Hao\"\n",
    "results = model_1000_100_state.retriever.get_relevant_documents(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - that all looks good - lets continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RAG Chain for the different models\n",
    "\n",
    "This will take a model_run_state that will pass in:\n",
    "- the qa model\n",
    "- the retriever \n",
    "\n",
    "It creates the RAG chain and saves it in the model_run_state for RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Created\n"
     ]
    }
   ],
   "source": [
    "from utilities.templates import get_qa_prompt\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from utilities.debugger import dprint\n",
    "\n",
    "def create_rag_chain(app_state, model_run_state):\n",
    "\n",
    "    chat_prompt = get_qa_prompt()\n",
    "\n",
    "    simple_chain = chat_prompt | model_run_state.qa_model\n",
    "    dprint(app_state, simple_chain.invoke({\"question\": \"Can you give me a summary of the 2 documents\", \"context\":\"\"}))\n",
    "\n",
    "    rag_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | model_run_state.retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "\n",
    "\n",
    "        | {\"response\": chat_prompt | model_run_state.qa_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    response = rag_qa_chain.invoke({\"question\" : \"What is the AI Bill of Rights \"})\n",
    "    dprint(app_state, response)\n",
    "    dprint(app_state, response[\"response\"].content)\n",
    "    dprint(app_state, f\"Number of found context: {len(response['context'])}\")\n",
    "    model_run_state.rag_qa_chain = rag_qa_chain\n",
    "    print(\"RAG Chain Created\")\n",
    "\n",
    "create_rag_chain(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG - Create the questions for evaluation\n",
    "\n",
    "3 functions are used to set up the questions:\n",
    "- batch_chunks - processes batches of chunks to try and get past the limitations with OpenAI quotas\n",
    "- create_chunks_for_ragas - takes the documents and splits them up based on the RagasState - this will allow us more experimentation later\n",
    "- create_questions_for_ragas - sets up the generator and creates the number of questions and distribution based on RagasState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be skipped since the questions are stored in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.ragas_state import RagasState\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "import time\n",
    "\n",
    "# create document chunks\n",
    "def Create_chunks_for_ragas(app_state, ragas_state):\n",
    "    # we have 2 documents so want representative across both\n",
    "    text_splitter_eval = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = ragas_state.chunk_size,\n",
    "        chunk_overlap = ragas_state.chunk_overlap,\n",
    "        length_function = len\n",
    "    )\n",
    "    combined_chunks_document = []\n",
    "    for document in app_state.documents:\n",
    "        eval_document = document[\"loaded_document\"]\n",
    "        document_chunks = text_splitter_eval.split_documents(eval_document)\n",
    "        print(f\"Num chumks: {len(document_chunks)}\")\n",
    "        combined_chunks_document = combined_chunks_document + document_chunks\n",
    "\n",
    "    print(f\"Total chunks: {len(combined_chunks_document)}\")\n",
    "    ragas_state.chunks = combined_chunks_document\n",
    "    print()\n",
    "\n",
    "# submit batches\n",
    "def batch_chunks(chunks, batch_size):\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        yield chunks[i:i + batch_size]\n",
    "\n",
    "# create the questions\n",
    "def create_questions_for_ragas(app_state, ragas_state):\n",
    "    generator_llm = ChatOpenAI(model=ragas_state.generator_llm)\n",
    "    critic_llm = ChatOpenAI(model=ragas_state.critic_llm)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    generator = TestsetGenerator.from_langchain(\n",
    "        generator_llm,\n",
    "        critic_llm,\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    batch_size = 200  # Number of chunks to process per batch\n",
    "    delay_between_batches = 1  # 1 second delay between batches\n",
    "\n",
    "    all_test_data = []\n",
    "\n",
    "    for batch in batch_chunks(ragas_state.chunks, batch_size):\n",
    "        print(f\"Processing batch of {len(batch)} chunks...\")\n",
    "\n",
    "        # Generate testset for the current batch\n",
    "        testset = generator.generate_with_langchain_docs(\n",
    "            batch,  # Process this batch of chunks\n",
    "            ragas_state.num_questions, \n",
    "            ragas_state.distributions\n",
    "        )\n",
    "\n",
    "        # Convert the testset to pandas and store the result\n",
    "        testset_df = testset.to_pandas()\n",
    "        all_test_data.append(testset_df)\n",
    "\n",
    "        # Wait 1 second before the next batch\n",
    "        time.sleep(delay_between_batches)\n",
    "\n",
    "\n",
    "\n",
    "    combined_testset_df = pd.concat(all_test_data, ignore_index=True)\n",
    "    ragas_state.testset_df = combined_testset_df\n",
    "\n",
    "    print(\"Ragas questions created for all batches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ragas_state if it exists offline \n",
    "\n",
    "Otherwise we will need to run the question creation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas state loaded from ragas_state.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# File path where ragas_state is stored\n",
    "file_path = 'ragas_state.pkl'\n",
    "\n",
    "# Check if the pickled file exists, and load it if it does\n",
    "def load_ragas_state_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                ragas_state = pickle.load(f)\n",
    "            print(f\"Ragas state loaded from {file_path}\")\n",
    "            return ragas_state\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ragas state: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No existing ragas state found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Use this to load ragas_state from pickle\n",
    "ragas_state = load_ragas_state_if_exists(file_path)\n",
    "ragas_state.distributions = {\n",
    "            simple: 0.5,\n",
    "            multi_context: 0.4,\n",
    "            reasoning: 0.1\n",
    "        }\n",
    "\n",
    "# use this to rebuild pickle state\n",
    "# ragas_state = RagasState()\n",
    "# ragas_state.generator_llm = \"gpt-4o\"\n",
    "# Create_chunks_for_ragas(app_state, ragas_state)\n",
    "# create_questions_for_ragas(app_state, ragas_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in testset_df: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of entries in testset_df: {ragas_state.testset_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save off the ragas state - then comment out above code so we don't need to run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the ragas_state without distributions\n",
    "def save_ragas_state(ragas_state, file_path):\n",
    "    # Temporarily remove `distributions` before saving\n",
    "    distributions_backup = ragas_state.distributions\n",
    "    ragas_state.distributions = None\n",
    "\n",
    "    # Save the rest of the object\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(ragas_state, f)\n",
    "        print(f\"Ragas state saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving ragas state: {e}\")\n",
    "    \n",
    "    # Restore `distributions` after saving\n",
    "    ragas_state.distributions = distributions_backup\n",
    "\n",
    "# Uncomment and run if need to save new ragas_state\n",
    "# save_ragas_state(ragas_state, 'ragas_state.pkl')\n",
    "\n",
    "# lets see if unpickle works\n",
    "# test_ragas_state = load_ragas_state_if_exists(file_path)\n",
    "# print(len(test_ragas_state.testset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of questions in an attempt to get past the OpenAI quota limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly removed 15 entries from ragas_state.testset_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is data collection emphasized as a necessa...</td>\n",
       "      <td>[In discussion of technical and governance int...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are some examples of protected classifica...</td>\n",
       "      <td>[different treatment or impacts disfavoring pe...</td>\n",
       "      <td>Some examples of protected classifications tha...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do hybrid AI-human platforms balance effic...</td>\n",
       "      <td>[chat-bots and AI-driven call response systems...</td>\n",
       "      <td>Hybrid AI-human platforms balance efficiency a...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What risks come from unregulated consumer data...</td>\n",
       "      <td>[of private collection.  \\nMeanwhile, members ...</td>\n",
       "      <td>The risks from unregulated consumer data colle...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What privacy risks are associated with data me...</td>\n",
       "      <td>[Models may leak, generate, or correctly infer...</td>\n",
       "      <td>Data memorization in LLMs may pose exacerbated...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "5   Why is data collection emphasized as a necessa...   \n",
       "6   What are some examples of protected classifica...   \n",
       "7   How do hybrid AI-human platforms balance effic...   \n",
       "8   What risks come from unregulated consumer data...   \n",
       "15  What privacy risks are associated with data me...   \n",
       "\n",
       "                                             contexts  \\\n",
       "5   [In discussion of technical and governance int...   \n",
       "6   [different treatment or impacts disfavoring pe...   \n",
       "7   [chat-bots and AI-driven call response systems...   \n",
       "8   [of private collection.  \\nMeanwhile, members ...   \n",
       "15  [Models may leak, generate, or correctly infer...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "5   The answer to given question is not present in...         simple   \n",
       "6   Some examples of protected classifications tha...         simple   \n",
       "7   Hybrid AI-human platforms balance efficiency a...  multi_context   \n",
       "8   The risks from unregulated consumer data colle...  multi_context   \n",
       "15  Data memorization in LLMs may pose exacerbated...         simple   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "5   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "6   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "7   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "8   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "15  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_random_entries_from_testset_df(ragas_state, num_entries=9):\n",
    "    if len(ragas_state.testset_df) >= num_entries:\n",
    "        rows_to_remove = ragas_state.testset_df.sample(n=num_entries).index\n",
    "        ragas_state.testset_df = ragas_state.testset_df.drop(rows_to_remove)\n",
    "        print(f\"Randomly removed {num_entries} entries from ragas_state.testset_df\")\n",
    "    else:\n",
    "        print(f\"Cannot remove {num_entries} entries, only {len(ragas_state.testset_df)} entries in the DataFrame.\")\n",
    "\n",
    "\n",
    "remove_random_entries_from_testset_df(ragas_state, num_entries=15)\n",
    "ragas_state.testset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers based on the pipeline we have created\n",
    "\n",
    "This function takes in the model_run_state and the ragas_state and uses the retriever from the model_run_state to answer the questions from the ragas_state. \n",
    "\n",
    "The response dataset is stored in the model_run_state for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers created - ready for Ragas evaluation\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def create_answers(app_state, model_run_state, ragas_state):\n",
    "  answers = []\n",
    "  contexts = []\n",
    "\n",
    "  test_questions = ragas_state.testset_df[\"question\"].values.tolist()\n",
    "  test_groundtruths = ragas_state.testset_df[\"ground_truth\"].values.tolist()\n",
    "\n",
    "  for question in test_questions:\n",
    "    response = model_run_state.rag_qa_chain.invoke({\"question\" : question})\n",
    "    answers.append(response[\"response\"].content)\n",
    "    contexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "  # Wrap it in a huggingface dataset\n",
    "  model_run_state.response_dataset = Dataset.from_dict({\n",
    "      \"question\" : test_questions,\n",
    "      \"answer\" : answers,\n",
    "      \"contexts\" : contexts,\n",
    "      \"ground_truth\" : test_groundtruths\n",
    "  })\n",
    "  model_run_state.response_dataset[0]\n",
    "  print(\"Answers created - ready for Ragas evaluation\")\n",
    "\n",
    "create_answers(app_state, model_1000_100_state, ragas_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The run_ragas_evaluation uses the response dataset from the previous step stored in the model_run_state to determine the requested Ragas metrics.\n",
    "\n",
    "The results of the evaluation are then stored nack in the model_run_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "def run_ragas_evaluation(app_state, model_run_state):\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_correctness,\n",
    "    ]\n",
    "    # model_run_state.ragas_results = evaluate(model_run_state.response_dataset, metrics)\n",
    "    # print(\"Ragas evaluation complete\")\n",
    "    results_dict = {}\n",
    "\n",
    "    # Evaluate each metric one by one\n",
    "    for metric in metrics:\n",
    "        metric_name = type(metric).__name__\n",
    "        print(f\"Evaluating metric: {metric_name}\")\n",
    "        try:\n",
    "            # Run the evaluation for the current metric\n",
    "            result = evaluate(model_run_state.response_dataset, [metric])\n",
    "            score = result.scores\n",
    "            results_dict[metric_name] = score\n",
    "            print(f\"Metric {metric_name} evaluation complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating metric {metric_name}: {e}\")\n",
    "    print(\"Adding results_dict to model_run_state\")\n",
    "    print(results_dict)\n",
    "    model_run_state.ragas_results_dict = results_dict\n",
    "    print(\"Ragas evaluation for all metrics complete\")     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the evaluation for the TE3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ragas_evaluation(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Model\n",
    "\n",
    "We can report on the model and the paramters such as the chunking size, overlap, and the Ragas metrics summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up functions to handle the display of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get the averages for all metrics in a single model\n",
    "def get_model_averages(model):\n",
    "    results = model.ragas_results_dict\n",
    "    averages = {}\n",
    "    \n",
    "    metric_mapping = {\n",
    "        'Faithfulness': 'faithfulness',\n",
    "        'AnswerRelevancy': 'answer_relevancy',\n",
    "        'ContextRecall': 'context_recall',\n",
    "        'ContextPrecision': 'context_precision',\n",
    "        'AnswerCorrectness': 'answer_correctness'\n",
    "    }\n",
    "    \n",
    "    for metric_name, dataset in results.items():\n",
    "        df = dataset.to_pandas()\n",
    "        # Get the corresponding column name from the mapping\n",
    "        metric_column = metric_mapping.get(metric_name, None)\n",
    "        if metric_column is None:\n",
    "            print(f\"Metric {metric_name} not found in the mapping.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            avg_value = df[metric_column].mean()  # Calculate the average for the specific metric column\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {metric_column} not found in the dataset. Available columns: {df.columns}\")\n",
    "            continue\n",
    "        averages[metric_name] = avg_value\n",
    "\n",
    "    return pd.DataFrame(averages.items(), columns=['Metric', 'Average'])\n",
    "\n",
    "def display_metrics(model_run_state):\n",
    "    \n",
    "    df = get_model_averages(model_run_state)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: text-embedding-3-small\n",
      "Chunk size: 1000\n",
      "Chunk overlap: 100\n",
      "              Metric   Average\n",
      "0       Faithfulness  0.940000\n",
      "1    AnswerRelevancy  0.977401\n",
      "2      ContextRecall  1.000000\n",
      "3   ContextPrecision  0.775980\n",
      "4  AnswerCorrectness  0.750321\n"
     ]
    }
   ],
   "source": [
    "model_1000_100_state.parameters()\n",
    "display_metrics(model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Snowflake Model Evaluation\n",
    "\n",
    "Process:\n",
    "- set up the model_run_state for the base Snowflake model\n",
    "- create the vector store using the base model\n",
    "- create the RAG chain with the retriever for the vestor store\n",
    "- generate the answers to the Ragas questions\n",
    "- evaluate the model's performance using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    }
   ],
   "source": [
    "snowflake_base_state = ModelRunState()\n",
    "snowflake_base_state.name = \"Snowflake_Base/1000/100\"\n",
    "snowflake_base_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_base_state.qa_model = ChatOpenAI(model=snowflake_base_state.qa_model_name)\n",
    "\n",
    "# snowflake embedding model\n",
    "snowflake_base_state.embedding_model_name = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "snowflake_base_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_base_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_base_state.chunk_size = 1000\n",
    "snowflake_base_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_base_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_base_state)\n",
    "create_answers(app_state, snowflake_base_state, ragas_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:17<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:26<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:22<00:00, 28.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Ragas evaluation for all metrics complete\n"
     ]
    }
   ],
   "source": [
    "run_ragas_evaluation(app_state, snowflake_base_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Snowflake model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: Snowflake/snowflake-arctic-embed-m\n",
      "Chunk size: 1000\n",
      "Chunk overlap: 100\n",
      "              Metric   Average\n",
      "0       Faithfulness  0.400000\n",
      "1    AnswerRelevancy  0.383450\n",
      "2      ContextRecall  0.550000\n",
      "3   ContextPrecision  0.203968\n",
      "4  AnswerCorrectness  0.332609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snowflake_base_state.parameters()\n",
    "display_metrics(snowflake_base_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Helper function to extract average values from a dataset\n",
    "def get_average_metric(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    return df.mean().iloc[0]  # Return the average of the first column (since each dataset only has one column)\n",
    "def compare_results(run_model_1, run_model_2):\n",
    "    # Extract the metrics for both models\n",
    "    results_1 = run_model_1.ragas_results_dict\n",
    "    results_2 = run_model_2.ragas_results_dict\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Metric': [],\n",
    "        run_model_1.name: [],\n",
    "        run_model_2.name: [],\n",
    "        'Difference': []\n",
    "    }\n",
    "\n",
    "    for metric_name in results_1.keys():\n",
    "        # Calculate the average for each metric\n",
    "        avg_1 = get_average_metric(results_1[metric_name])\n",
    "        avg_2 = get_average_metric(results_2[metric_name])\n",
    "        \n",
    "        # Append to the comparison data\n",
    "        comparison_data['Metric'].append(metric_name)\n",
    "        comparison_data[run_model_1.name].append(avg_1)\n",
    "        comparison_data[run_model_2.name].append(avg_2)\n",
    "        comparison_data['Difference'].append(avg_2 - avg_1)\n",
    "\n",
    "    return pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Base Snowflake with the TE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Metric  Snowflake_Base/1000/100  TE3/1000/100  Difference\n",
      "0       Faithfulness                 0.400000      0.940000    0.540000\n",
      "1    AnswerRelevancy                 0.383450      0.977401    0.593951\n",
      "2      ContextRecall                 0.550000      1.000000    0.450000\n",
      "3   ContextPrecision                 0.203968      0.775980    0.572012\n",
      "4  AnswerCorrectness                 0.332609      0.750321    0.417711\n"
     ]
    }
   ],
   "source": [
    "df = compare_results(snowflake_base_state, model_1000_100_state)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "snowflake_finetune_state = ModelRunState()\n",
    "snowflake_finetune_state.name = \"Snowflake_Fine/1000/100\"\n",
    "snowflake_finetune_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_state.qa_model = ChatOpenAI(model=snowflake_finetune_state.qa_model_name)\n",
    "\n",
    "# finetune snowflake embedding model\n",
    "\n",
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model\"\n",
    "\n",
    "# Load the fine-tuned model from Hugging Face\n",
    "snowflake_finetune_state.embedding_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_state.chunk_size = 1000\n",
    "snowflake_finetune_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_state)\n",
    "create_answers(app_state, snowflake_finetune_state, ragas_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:05<00:00, 25.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:48<00:00,  9.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:16<00:00, 27.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Ragas evaluation for all metrics complete\n"
     ]
    }
   ],
   "source": [
    "run_ragas_evaluation(app_state, snowflake_finetune_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: rchrdgwr/finetuned-arctic-model\n",
      "Chunk size: 1000\n",
      "Chunk overlap: 100\n",
      "              Metric   Average\n",
      "0       Faithfulness  1.000000\n",
      "1    AnswerRelevancy  0.973854\n",
      "2      ContextRecall  1.000000\n",
      "3   ContextPrecision       NaN\n",
      "4  AnswerCorrectness  0.632057\n"
     ]
    }
   ],
   "source": [
    "snowflake_finetune_state.parameters()\n",
    "display_metrics(snowflake_finetune_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of TE3, Base Snowflake, and Fine Tuned Snowflake (1st Model) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def compare_results_3(run_model_1, run_model_2, run_model_3):\n",
    "    # Extract results for each model\n",
    "    results_1 = run_model_1.ragas_results_dict\n",
    "    results_2 = run_model_2.ragas_results_dict\n",
    "    results_3 = run_model_3.ragas_results_dict\n",
    "\n",
    "    comparison_data = {\n",
    "        'Metric': [],\n",
    "        run_model_1.name: [],\n",
    "        run_model_2.name: [],\n",
    "        run_model_3.name: [],\n",
    "        '1v2 Difference': [],\n",
    "        '1v3 Difference': [],\n",
    "        '2v3 Difference': []\n",
    "    }\n",
    "\n",
    "    for metric_name in results_1.keys():\n",
    "        # Calculate the average for each metric\n",
    "        avg_1 = get_average_metric(results_1[metric_name])\n",
    "        avg_2 = get_average_metric(results_2[metric_name])\n",
    "        avg_3 = get_average_metric(results_3[metric_name])\n",
    "        \n",
    "        # Append to the comparison data\n",
    "        comparison_data['Metric'].append(metric_name)\n",
    "        comparison_data[run_model_1.name].append(avg_1)\n",
    "        comparison_data[run_model_2.name].append(avg_2)\n",
    "        comparison_data[run_model_3.name].append(avg_3)\n",
    "        comparison_data['1v2 Difference'].append(avg_2 - avg_1)\n",
    "        comparison_data['1v3 Difference'].append(avg_3 - avg_1)\n",
    "        comparison_data['2v3 Difference'].append(avg_3 - avg_2)\n",
    "\n",
    "    return pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>TE3/1000/100</th>\n",
       "      <th>Snowflake_Base/1000/100</th>\n",
       "      <th>Snowflake_Fine/1000/100</th>\n",
       "      <th>1v2 Difference</th>\n",
       "      <th>1v3 Difference</th>\n",
       "      <th>2v3 Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnswerRelevancy</td>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.383450</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>-0.593951</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>0.590404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ContextRecall</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContextPrecision</td>\n",
       "      <td>0.775980</td>\n",
       "      <td>0.203968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.572012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnswerCorrectness</td>\n",
       "      <td>0.750321</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.632057</td>\n",
       "      <td>-0.417711</td>\n",
       "      <td>-0.118264</td>\n",
       "      <td>0.299448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  TE3/1000/100  Snowflake_Base/1000/100  \\\n",
       "0       Faithfulness      0.940000                 0.400000   \n",
       "1    AnswerRelevancy      0.977401                 0.383450   \n",
       "2      ContextRecall      1.000000                 0.550000   \n",
       "3   ContextPrecision      0.775980                 0.203968   \n",
       "4  AnswerCorrectness      0.750321                 0.332609   \n",
       "\n",
       "   Snowflake_Fine/1000/100  1v2 Difference  1v3 Difference  2v3 Difference  \n",
       "0                 1.000000       -0.540000        0.060000        0.600000  \n",
       "1                 0.973854       -0.593951       -0.003547        0.590404  \n",
       "2                 1.000000       -0.450000        0.000000        0.450000  \n",
       "3                      NaN       -0.572012             NaN             NaN  \n",
       "4                 0.632057       -0.417711       -0.118264        0.299448  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = compare_results_3(model_1000_100_state , snowflake_base_state,  snowflake_finetune_state)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets run some different chunking strategies\n",
    "\n",
    "Section Aware\n",
    "\n",
    "Table Aware\n",
    "\n",
    "Semantic Chunking\n",
    "\n",
    "Then run a comparison with the base that used recursive text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model\"\n",
    "\n",
    "snowflake_finetune_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Section Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n",
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:11<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:02<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating:  20%|██        | 1/5 [02:59<11:59, 180.00s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [02:59<00:00, 36.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:12<00:00, 26.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Adding results_dict to model_run_state\n",
      "{'Faithfulness': Dataset({\n",
      "    features: ['faithfulness'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerRelevancy': Dataset({\n",
      "    features: ['answer_relevancy'],\n",
      "    num_rows: 5\n",
      "}), 'ContextRecall': Dataset({\n",
      "    features: ['context_recall'],\n",
      "    num_rows: 5\n",
      "}), 'ContextPrecision': Dataset({\n",
      "    features: ['context_precision'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerCorrectness': Dataset({\n",
      "    features: ['answer_correctness'],\n",
      "    num_rows: 5\n",
      "})}\n",
      "Ragas evaluation for all metrics complete\n"
     ]
    }
   ],
   "source": [
    "from utilities.constants import (\n",
    "    CHUNKING_STRATEGY_TABLE_AWARE,\n",
    "    CHUNKING_STRATEGY_SECTION_BASED,\n",
    "    CHUNKING_STRATEGY_SEMANTIC\n",
    ")\n",
    "\n",
    "snowflake_finetune_section_state = ModelRunState()\n",
    "snowflake_finetune_section_state.name = \"Snowflake_FineSection/1000/100\"\n",
    "snowflake_finetune_section_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_section_state.qa_model = ChatOpenAI(model=snowflake_finetune_section_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_section_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_section_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_section_state.chunking_strategy = CHUNKING_STRATEGY_SECTION_BASED\n",
    "snowflake_finetune_section_state.chunk_size = 1000\n",
    "snowflake_finetune_section_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_section_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_section_state)\n",
    "create_answers(app_state, snowflake_finetune_section_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_section_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Metric   Average\n",
      "0       Faithfulness  0.928342\n",
      "1    AnswerRelevancy  0.963411\n",
      "2      ContextRecall  0.900000\n",
      "3   ContextPrecision       NaN\n",
      "4  AnswerCorrectness  0.466659\n"
     ]
    }
   ],
   "source": [
    "display_metrics(snowflake_finetune_section_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Table Aware Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n",
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [01:00<00:00, 12.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:38<00:00,  7.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating:  20%|██        | 1/5 [03:00<12:00, 180.00s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:16<00:00, 27.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Adding results_dict to model_run_state\n",
      "{'Faithfulness': Dataset({\n",
      "    features: ['faithfulness'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerRelevancy': Dataset({\n",
      "    features: ['answer_relevancy'],\n",
      "    num_rows: 5\n",
      "}), 'ContextRecall': Dataset({\n",
      "    features: ['context_recall'],\n",
      "    num_rows: 5\n",
      "}), 'ContextPrecision': Dataset({\n",
      "    features: ['context_precision'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerCorrectness': Dataset({\n",
      "    features: ['answer_correctness'],\n",
      "    num_rows: 5\n",
      "})}\n",
      "Ragas evaluation for all metrics complete\n",
      "              Metric   Average\n",
      "0       Faithfulness  0.890942\n",
      "1    AnswerRelevancy  0.969810\n",
      "2      ContextRecall  1.000000\n",
      "3   ContextPrecision       NaN\n",
      "4  AnswerCorrectness  0.380591\n"
     ]
    }
   ],
   "source": [
    "snowflake_finetune_table_state = ModelRunState()\n",
    "snowflake_finetune_table_state.name = \"Snowflake_FineTable/1000/100\"\n",
    "snowflake_finetune_table_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_table_state.qa_model = ChatOpenAI(model=snowflake_finetune_table_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_table_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_table_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_table_state.chunking_strategy = CHUNKING_STRATEGY_TABLE_AWARE\n",
    "snowflake_finetune_table_state.chunk_size = 1000\n",
    "snowflake_finetune_table_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_table_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_table_state)\n",
    "create_answers(app_state, snowflake_finetune_table_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_table_state)\n",
    "display_metrics(snowflake_finetune_table_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Semantic Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n",
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Evaluating:  20%|██        | 1/5 [03:00<12:00, 180.00s/it]Exception raised in Job[3]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:13<00:00, 26.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Adding results_dict to model_run_state\n",
      "{'Faithfulness': Dataset({\n",
      "    features: ['faithfulness'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerRelevancy': Dataset({\n",
      "    features: ['answer_relevancy'],\n",
      "    num_rows: 5\n",
      "}), 'ContextRecall': Dataset({\n",
      "    features: ['context_recall'],\n",
      "    num_rows: 5\n",
      "}), 'ContextPrecision': Dataset({\n",
      "    features: ['context_precision'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerCorrectness': Dataset({\n",
      "    features: ['answer_correctness'],\n",
      "    num_rows: 5\n",
      "})}\n",
      "Ragas evaluation for all metrics complete\n",
      "              Metric   Average\n",
      "0       Faithfulness  0.798095\n",
      "1    AnswerRelevancy  0.967873\n",
      "2      ContextRecall  0.500000\n",
      "3   ContextPrecision       NaN\n",
      "4  AnswerCorrectness  0.604548\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snowflake_finetune_semantic_state = ModelRunState()\n",
    "snowflake_finetune_semantic_state.name = \"Snowflake_FineSemantic/1000/100\"\n",
    "snowflake_finetune_semantic_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_semantic_state.qa_model = ChatOpenAI(model=snowflake_finetune_semantic_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_semantic_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_semantic_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_semantic_state.chunking_strategy = CHUNKING_STRATEGY_SEMANTIC\n",
    "snowflake_finetune_semantic_state.chunk_size = 1000\n",
    "snowflake_finetune_semantic_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_semantic_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_semantic_state)\n",
    "create_answers(app_state, snowflake_finetune_semantic_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_semantic_state)\n",
    "display_metrics(snowflake_finetune_semantic_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Fine Tuned Snowflake Model (1st Model) with 3 Different Chunking Strategies\n",
    "\n",
    "Note the Fine Tuned model used simple recursive text with specified chunking size and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Snowflake_Fine/1000/100</th>\n",
       "      <th>Snowflake_FineSection/1000/100</th>\n",
       "      <th>Snowflake_FineTable/1000/100</th>\n",
       "      <th>Snowflake_FineSemantic/1000/100</th>\n",
       "      <th>1v2 Difference</th>\n",
       "      <th>1v3 Difference</th>\n",
       "      <th>1v4 Difference</th>\n",
       "      <th>2v3 Difference</th>\n",
       "      <th>2v4 Difference</th>\n",
       "      <th>3v4 Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928342</td>\n",
       "      <td>0.890942</td>\n",
       "      <td>0.798095</td>\n",
       "      <td>-0.071658</td>\n",
       "      <td>-0.109058</td>\n",
       "      <td>-0.201905</td>\n",
       "      <td>-0.037400</td>\n",
       "      <td>-0.130247</td>\n",
       "      <td>-0.092847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnswerRelevancy</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>0.963411</td>\n",
       "      <td>0.969810</td>\n",
       "      <td>0.967873</td>\n",
       "      <td>-0.010443</td>\n",
       "      <td>-0.004044</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.004462</td>\n",
       "      <td>-0.001937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ContextRecall</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContextPrecision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnswerCorrectness</td>\n",
       "      <td>0.632057</td>\n",
       "      <td>0.466659</td>\n",
       "      <td>0.380591</td>\n",
       "      <td>0.604548</td>\n",
       "      <td>-0.165398</td>\n",
       "      <td>-0.251467</td>\n",
       "      <td>-0.027509</td>\n",
       "      <td>-0.086068</td>\n",
       "      <td>0.137889</td>\n",
       "      <td>0.223958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  Snowflake_Fine/1000/100  Snowflake_FineSection/1000/100  \\\n",
       "0       Faithfulness                 1.000000                        0.928342   \n",
       "1    AnswerRelevancy                 0.973854                        0.963411   \n",
       "2      ContextRecall                 1.000000                        0.900000   \n",
       "3   ContextPrecision                      NaN                             NaN   \n",
       "4  AnswerCorrectness                 0.632057                        0.466659   \n",
       "\n",
       "   Snowflake_FineTable/1000/100  Snowflake_FineSemantic/1000/100  \\\n",
       "0                      0.890942                         0.798095   \n",
       "1                      0.969810                         0.967873   \n",
       "2                      1.000000                         0.500000   \n",
       "3                           NaN                              NaN   \n",
       "4                      0.380591                         0.604548   \n",
       "\n",
       "   1v2 Difference  1v3 Difference  1v4 Difference  2v3 Difference  \\\n",
       "0       -0.071658       -0.109058       -0.201905       -0.037400   \n",
       "1       -0.010443       -0.004044       -0.005981        0.006399   \n",
       "2       -0.100000        0.000000       -0.500000        0.100000   \n",
       "3             NaN             NaN             NaN             NaN   \n",
       "4       -0.165398       -0.251467       -0.027509       -0.086068   \n",
       "\n",
       "   2v4 Difference  3v4 Difference  \n",
       "0       -0.130247       -0.092847  \n",
       "1        0.004462       -0.001937  \n",
       "2       -0.400000       -0.500000  \n",
       "3             NaN             NaN  \n",
       "4        0.137889        0.223958  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_results_4(run_model_1, run_model_2, run_model_3, run_model_4):\n",
    "    # Extract results for each model\n",
    "    results_1 = run_model_1.ragas_results_dict\n",
    "    results_2 = run_model_2.ragas_results_dict\n",
    "    results_3 = run_model_3.ragas_results_dict\n",
    "    results_4 = run_model_4.ragas_results_dict\n",
    "\n",
    "    comparison_data = {\n",
    "        'Metric': [],\n",
    "        run_model_1.name: [],\n",
    "        run_model_2.name: [],\n",
    "        run_model_3.name: [],\n",
    "        run_model_4.name: [],\n",
    "        '1v2 Difference': [],\n",
    "        '1v3 Difference': [],\n",
    "        '1v4 Difference': [],\n",
    "        '2v3 Difference': [],\n",
    "        '2v4 Difference': [],\n",
    "        '3v4 Difference': []\n",
    "    }\n",
    "\n",
    "    for metric_name in results_1.keys():\n",
    "        # Calculate the average for each metric\n",
    "        avg_1 = get_average_metric(results_1[metric_name])\n",
    "        avg_2 = get_average_metric(results_2[metric_name])\n",
    "        avg_3 = get_average_metric(results_3[metric_name])\n",
    "        avg_4 = get_average_metric(results_4[metric_name])\n",
    "        \n",
    "        # Append to the comparison data\n",
    "        comparison_data['Metric'].append(metric_name)\n",
    "        comparison_data[run_model_1.name].append(avg_1)\n",
    "        comparison_data[run_model_2.name].append(avg_2)\n",
    "        comparison_data[run_model_3.name].append(avg_3)\n",
    "        comparison_data[run_model_4.name].append(avg_4)\n",
    "        comparison_data['1v2 Difference'].append(avg_2 - avg_1)\n",
    "        comparison_data['1v3 Difference'].append(avg_3 - avg_1)\n",
    "        comparison_data['1v4 Difference'].append(avg_4 - avg_1)\n",
    "        comparison_data['2v3 Difference'].append(avg_3 - avg_2)\n",
    "        comparison_data['2v4 Difference'].append(avg_4 - avg_2)\n",
    "        comparison_data['3v4 Difference'].append(avg_4 - avg_3)\n",
    "\n",
    "    return pd.DataFrame(comparison_data)\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "df = compare_results_4(snowflake_finetune_state , snowflake_finetune_section_state, snowflake_finetune_table_state, snowflake_finetune_semantic_state)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (2nd Model Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model-2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "snowflake_finetune_2_state = ModelRunState()\n",
    "snowflake_finetune_2_state.name = \"Snowflake_Fine_2/1000/100\"\n",
    "snowflake_finetune_2_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_2_state.qa_model = ChatOpenAI(model=snowflake_finetune_2_state.qa_model_name)\n",
    "\n",
    "# finetune snowflake embedding model\n",
    "\n",
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model-2\"\n",
    "\n",
    "# Load the fine-tuned model from Hugging Face\n",
    "snowflake_finetune_2_state.embedding_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_2_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_2_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_2_state.chunk_size = 1000\n",
    "snowflake_finetune_2_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_2_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_2_state)\n",
    "create_answers(app_state, snowflake_finetune_2_state, ragas_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metric: Faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:26<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Faithfulness evaluation complete\n",
      "Evaluating metric: AnswerRelevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerRelevancy evaluation complete\n",
      "Evaluating metric: ContextRecall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextRecall evaluation complete\n",
      "Evaluating metric: ContextPrecision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 2/5 [02:24<03:18, 66.28s/it] Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:01<00:00, 36.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ContextPrecision evaluation complete\n",
      "Evaluating metric: AnswerCorrectness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [02:14<00:00, 26.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric AnswerCorrectness evaluation complete\n",
      "Adding results_dict to model_run_state\n",
      "{'Faithfulness': Dataset({\n",
      "    features: ['faithfulness'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerRelevancy': Dataset({\n",
      "    features: ['answer_relevancy'],\n",
      "    num_rows: 5\n",
      "}), 'ContextRecall': Dataset({\n",
      "    features: ['context_recall'],\n",
      "    num_rows: 5\n",
      "}), 'ContextPrecision': Dataset({\n",
      "    features: ['context_precision'],\n",
      "    num_rows: 5\n",
      "}), 'AnswerCorrectness': Dataset({\n",
      "    features: ['answer_correctness'],\n",
      "    num_rows: 5\n",
      "})}\n",
      "Ragas evaluation for all metrics complete\n"
     ]
    }
   ],
   "source": [
    "run_ragas_evaluation(app_state, snowflake_finetune_2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Metric   Average\n",
      "0       Faithfulness  0.992000\n",
      "1    AnswerRelevancy  0.981830\n",
      "2      ContextRecall  1.000000\n",
      "3   ContextPrecision  0.751185\n",
      "4  AnswerCorrectness  0.526832\n"
     ]
    }
   ],
   "source": [
    "display_metrics(snowflake_finetune_2_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Two Fine Tuned Snowflake Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Snowflake_Fine/1000/100</th>\n",
       "      <th>Snowflake_Fine_2/1000/100</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>-0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnswerRelevancy</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>0.981830</td>\n",
       "      <td>0.007977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ContextRecall</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContextPrecision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.751185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnswerCorrectness</td>\n",
       "      <td>0.632057</td>\n",
       "      <td>0.526832</td>\n",
       "      <td>-0.105225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  Snowflake_Fine/1000/100  Snowflake_Fine_2/1000/100  \\\n",
       "0       Faithfulness                 1.000000                   0.992000   \n",
       "1    AnswerRelevancy                 0.973854                   0.981830   \n",
       "2      ContextRecall                 1.000000                   1.000000   \n",
       "3   ContextPrecision                      NaN                   0.751185   \n",
       "4  AnswerCorrectness                 0.632057                   0.526832   \n",
       "\n",
       "   Difference  \n",
       "0   -0.008000  \n",
       "1    0.007977  \n",
       "2    0.000000  \n",
       "3         NaN  \n",
       "4   -0.105225  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = compare_results(snowflake_finetune_state, snowflake_finetune_2_state )\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
