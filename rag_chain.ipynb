{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineering Bootcamp Cohort 4 Midterm\n",
    "\n",
    "For details of this project please see the project Readme.\n",
    "\n",
    "This notebooks takes advantage of shared state classes and some common utility functions shared with the Chainlit application and other notebooks\n",
    "\n",
    "The state classes are:\n",
    "- AppState - contains information about the application and the documents we are looking at\n",
    "- ModelRunState - contains information for each individual run for a model, chunking parameters, and results \n",
    "- RagasState - contains information for Ragas evaluation including the questions and context\n",
    "\n",
    "The state is initialized and is passed between the functions. Each model/chunking strategy/parameters can run and save their information and then can be easily compared. \n",
    "\n",
    "This utilities include:\n",
    "- constants.py - constants used throughout the package (mainly for the chunking strategies)\n",
    "- debugger.py - supports printing messages for when debug=True\n",
    "- doc_utilities.py - supports reading the documents\n",
    "- rag_utilities.py - supports the creation of the RAG chain \n",
    "- templates.py - provides templates for RAG chains\n",
    "- vector_utilities - provides functions for chunking documents and setting up the vector store. Includes 4 chunking strategies:\n",
    "    - Recursive splitter with chunk size and overlap\n",
    "    - Table aware - tries to handle pdfs with tables\n",
    "    - Section based - chunks based on section headers\n",
    "    - Semantic splitter - semantic-based chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow for async in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps hide errors from using Hugging Face's transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install our key components for RAG etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q langchain-core==0.2.27 langchain-community==0.2.10\n",
    "!pip install -q langchain-experimental==0.0.64 langgraph-checkpoint==1.0.6 langgraph==0.2.16 langchain-qdrant==0.1.3\n",
    "!pip install -q langchain-openai==0.1.9\n",
    "!pip install -q ragas==0.1.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install our vector store - Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU qdrant-client==1.11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install supporting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU tiktoken==0.7.0 pymupdf==1.24.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Variables\n",
    "\n",
    "- OpenAI API Key - will use some of the OpenAI models - if in .env use it otherwise ask for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#if not openai_api_key:\n",
    "openai_api_key = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our starting inputs and state and read in the documents\n",
    "\n",
    "The AppState contains the docs to be used throughout the process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 2\n"
     ]
    }
   ],
   "source": [
    "from classes.app_state import AppState\n",
    "from utilities.doc_utilities import get_documents\n",
    "document_urls = [\n",
    "    \"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\",\n",
    "    \"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\",\n",
    "]\n",
    "\n",
    "app_state = AppState()\n",
    "app_state.set_debug(False)\n",
    "\n",
    "app_state.set_document_urls(document_urls)\n",
    "\n",
    "get_documents(app_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store with text-embedding-3-small embeddings\n",
    "\n",
    "Set up our first model run - using the text-embedding-3-small for embeddings initially\n",
    "\n",
    "This will test that our chunking and vector store functions are all working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n"
     ]
    }
   ],
   "source": [
    "from classes.model_run_state import ModelRunState\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from utilities.vector_utilities import create_vector_store\n",
    "\n",
    "model_1000_100_state = ModelRunState()\n",
    "model_1000_100_state.name = \"TE3/1000/100\"\n",
    "model_1000_100_state.chunk_size = 1000\n",
    "model_1000_100_state.chunk_overlap = 100\n",
    "\n",
    "model_1000_100_state.qa_model_name = \"gpt-4o-mini\"\n",
    "model_1000_100_state.qa_model = ChatOpenAI(model=model_1000_100_state.qa_model_name)\n",
    "\n",
    "# the openai embedding model\n",
    "model_1000_100_state.embedding_model_name = \"text-embedding-3-small\"\n",
    "model_1000_100_state.embedding_model = OpenAIEmbeddings(model=model_1000_100_state.embedding_model_name)\n",
    "\n",
    "create_vector_store(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the retriever with some sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "You should be protected from abusive data practices via built-in \n",
      "protections and you should have agency over how data about \n",
      "you is used. You should be protected from violations of privacy through \n",
      "design choices that ensure such protections are included by default, including \n",
      "ensuring that data collection conforms to reasonable expectations and that \n",
      "only data strictly necessary for the specific context is collected. Designers, de­\n",
      "velopers, and deployers of automated systems should seek your permission \n",
      "and respect your decisions regarding collection, use, access, transfer, and de­\n",
      "letion of your data in appropriate ways and to the greatest extent possible; \n",
      "where not possible, alternative privacy by design safeguards should be used. \n",
      "Systems should not employ user experience and design decisions that obfus­\n",
      "cate user choice or burden users with defaults that are privacy invasive. Con­\n",
      "sent should only be used to justify collection of data in cases where it can be \n",
      "appropriately and meaningfully given. Any consent requests should be brief, \n",
      "be understandable in plain language, and give you agency over data collection \n",
      "and the specific context of use; current hard-to-understand no­\n",
      "tice-and-choice practices for broad uses of data should be changed. Enhanced \n",
      "protections and restrictions for data and inferences related to sensitive do­\n",
      "mains, including health, work, education, criminal justice, and finance, and \n",
      "for data pertaining to youth should put you first. In sensitive domains, your \n",
      "data and related inferences should only be used for necessary functions, and \n",
      "you should be protected by ethical review and use prohibitions. You and your \n",
      "communities should be free from unchecked surveillance; surveillance tech­\n",
      "nologies should be subject to heightened oversight that includes at least \n",
      "pre-deployment assessment of their potential harms and scope limits to pro­\n",
      "tect privacy and civil liberties. Continuous surveillance and monitoring \n",
      "should not be used in education, work, housing, or in other contexts where the \n",
      "use of such surveillance technologies is likely to limit rights, opportunities, or \n",
      "access. Whenever possible, you should have access to reporting that confirms \n",
      "your data decisions have been respected and provides an assessment of the \n",
      "potential impact of surveillance technologies on your rights, opportunities, or \n",
      "access. \n",
      "DATA PRIVACY\n",
      "30\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 25, '_id': '73eef6f8903c4092a6702b6f2365ca5f', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How should you be protected from abusive data practices \"\n",
    "results = model_1000_100_state.retriever.get_relevant_documents(query)\n",
    "\n",
    "print(len(results))\n",
    "print(results[0].page_content)\n",
    "print(results[0].metadata)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDNOTES\n",
      "75. See., e.g., Sam Sabin. Digital surveillance in a post-Roe world. Politico. May 5, 2022. https://\n",
      "www.politico.com/newsletters/digital-future-daily/2022/05/05/digital-surveillance-in-a-post-roe­\n",
      "world-00030459; Federal Trade Commission. FTC Sues Kochava for Selling Data that Tracks People at\n",
      "Reproductive Health Clinics, Places of Worship, and Other Sensitive Locations. Aug. 29, 2022. https://\n",
      "www.ftc.gov/news-events/news/press-releases/2022/08/ftc-sues-kochava-selling-data-tracks-people­\n",
      "reproductive-health-clinics-places-worship-other\n",
      "76. Todd Feathers. This Private Equity Firm Is Amassing Companies That Collect Data on America’s\n",
      "Children. The Markup. Jan. 11, 2022.\n",
      "https://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies­\n",
      "that-collect-data-on-americas-children\n",
      "77. Reed Albergotti. Every employee who leaves Apple becomes an ‘associate’: In job databases used by\n",
      "employers to verify resume information, every former Apple employee’s title gets erased and replaced with\n",
      "a generic title. The Washington Post. Feb. 10, 2022.\n",
      "https://www.washingtonpost.com/technology/2022/02/10/apple-associate/\n",
      "78. National Institute of Standards and Technology. Privacy Framework Perspectives and Success\n",
      "Stories. Accessed May 2, 2022.\n",
      "https://www.nist.gov/privacy-framework/getting-started-0/perspectives-and-success-stories\n",
      "79. ACLU of New York. What You Need to Know About New York’s Temporary Ban on Facial\n",
      "Recognition in Schools. Accessed May 2, 2022.\n",
      "https://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial­\n",
      "recognition-schools\n",
      "80. New York State Assembly. Amendment to Education Law. Enacted Dec. 22, 2020.\n",
      "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=S05140&term=2019&Summary=Y&Text=Y\n",
      "81. U.S Department of Labor. Labor-Management Reporting and Disclosure Act of 1959, As Amended.\n",
      "https://www.dol.gov/agencies/olms/laws/labor-management-reporting-and-disclosure-act (Section\n",
      "203). See also: U.S Department of Labor. Form LM-10. OLMS Fact Sheet, Accessed May 2, 2022. https://\n",
      "www.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf\n",
      "82. See, e.g., Apple. Protecting the User’s Privacy. Accessed May 2, 2022.\n",
      "https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.\n",
      "Design for Safety: Android is secure by default and private by design. Accessed May 3, 2022.\n",
      "https://developer.android.com/design-for-safety\n",
      "83. Karen Hao. The coming war on the hidden algorithms that trap people in poverty. MIT Tech Review.\n",
      "Dec. 4, 2020.\n",
      "https://www.technologyreview.com/2020/12/04/1013068/algorithms-create-a-poverty-trap-lawyers­\n",
      "fight-back/\n",
      "84. Anjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers. Family Surveillance by Algorithm.\n",
      "ACLU. Accessed May 2, 2022.\n",
      "https://www.aclu.org/fact-sheet/family-surveillance-algorithm\n",
      "70\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 58, '_id': '9bf8a25a7661434097a3a43b44b28579', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "ENDNOTES\n",
      "57. ISO Technical Management Board. ISO/IEC Guide 71:2014. Guide for addressing accessibility in\n",
      "standards. International Standards Organization. 2021. https://www.iso.org/standard/57385.html\n",
      "58. World Wide Web Consortium. Web Content Accessibility Guidelines (WCAG) 2.0. Dec. 11, 2008.\n",
      "https://www.w3.org/TR/WCAG20/\n",
      "59. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert. NIST Special\n",
      "Publication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. The\n",
      "National Institute of Standards and Technology. March, 2022. https://nvlpubs.nist.gov/nistpubs/\n",
      "SpecialPublications/NIST.SP.1270.pdf\n",
      "60. See, e.g., the 2014 Federal Trade Commission report “Data Brokers A Call for Transparency and\n",
      "Accountability”. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency­\n",
      "accountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf\n",
      "61. See, e.g., Nir Kshetri. School surveillance of students via laptops may do more harm than good. The\n",
      "Conversation. Jan. 21, 2022.\n",
      "https://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than­\n",
      "good-170983; Matt Scherer. Warning: Bossware May be Hazardous to Your Health. Center for Democracy\n",
      "& Technology Report.\n",
      "https://cdt.org/wp-content/uploads/2021/07/2021-07-29-Warning-Bossware-May-Be-Hazardous-To­\n",
      "Your-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon\n",
      "Warehouses. HIP and WWRC report. Jan. 2021.\n",
      "https://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon­\n",
      "Warehouses-HIP-WWRC-01-21.pdf; Drew Harwell. Contract lawyers face a growing invasion of\n",
      "surveillance programs that monitor their work. The Washington Post. Nov. 11, 2021. https://\n",
      "www.washingtonpost.com/technology/2021/11/11/lawyer-facial-recognition-monitoring/;\n",
      "Virginia Doellgast and Sean O'Brady. Making Call Center Jobs Better: The Relationship between\n",
      "Management Practices and Worker Stress. A Report for the CWA. June 2020. https://\n",
      "hdl.handle.net/1813/74307\n",
      "62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May\n",
      "2014.\n",
      "https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability­\n",
      "report-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O’Neil.\n",
      "Weapons of Math Destruction. Penguin Books. 2017.\n",
      "https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction\n",
      "63. See, e.g., Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel. Social Media Surveillance by\n",
      "the U.S. Government. Brennan Center for Justice. Jan. 7, 2022.\n",
      "https://www.brennancenter.org/our-work/research-reports/social-media-surveillance-us-government;\n",
      "Shoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of\n",
      "Power. Public Affairs. 2019.\n",
      "64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.\n",
      "7, 2019.\n",
      "https://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big­\n",
      "data-discrimination-online-records\n",
      "68\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 56, '_id': '902dc81d840047729303f9484a9b5116', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "60 \n",
      "Zhang, Y. et al. (2023) Human favoritism, not AI aversion: People’s perceptions (and bias) toward \n",
      "generative AI, human experts, and human–GAI collaboration in persuasive content generation. Judgment \n",
      "and Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\n",
      "making/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\n",
      "human-experts-and-humangai-collaboration-in-persuasive-content-\n",
      "generation/419C4BD9CE82673EAF1D8F6C350C4FA8 \n",
      "Zhang, Y. et al. (2023) Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \n",
      "arXiv. https://arxiv.org/pdf/2309.01219 \n",
      "Zhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \n",
      "https://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\n",
      "Ananth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc\n",
      "{'source': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'document_id': '2b9b4525-a057-4952-9829-1c50afd5ecd2', 'chunk_number': 53, '_id': 'dedc7a8a170a4bf480a7e1ce8c26ea46', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "APPENDIX\n",
      "Panel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \n",
      "technology that impact equity of opportunity in employment, education, and housing. \n",
      "Welcome: \n",
      "•\n",
      "Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "Dominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic\n",
      "Studies\n",
      "Moderator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \n",
      "Panelists: \n",
      "•\n",
      "Christo Wilson, Associate Professor of Computer Science, Northeastern University\n",
      "•\n",
      "Frida Polli, CEO, Pymetrics\n",
      "•\n",
      "Karen Levy, Assistant Professor, Department of Information Science, Cornell University\n",
      "•\n",
      "Natasha Duarte, Project Director, Upturn\n",
      "•\n",
      "Elana Zeide, Assistant Professor, University of Nebraska College of Law\n",
      "•\n",
      "Fabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\n",
      "Advocate and Floor Captain, Atlantic Plaza Towers Tenants Association\n",
      "The individual panelists described the ways in which AI systems and other technologies are increasingly being \n",
      "used to limit access to equal opportunities in education, housing, and employment. Education-related \n",
      "concerning uses included the increased use of remote proctoring systems, student location and facial \n",
      "recognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses \n",
      "including automated tenant background screening and facial recognition-based controls to enter or exit \n",
      "housing complexes. Employment-related concerning uses included discrimination in automated hiring \n",
      "screening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \n",
      "concern, pointing out that students should be able to reinvent themselves and require privacy of their student \n",
      "records and education-related data in order to do so. The overarching concerns of surveillance in these \n",
      "domains included concerns about the chilling effects of surveillance on student expression, inappropriate \n",
      "control of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work \n",
      "and life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \n",
      "pointed out ways that data from one situation was misapplied in another in a way that limited people's \n",
      "opportunities, for example data from criminal justice settings or previous evictions being used to block further \n",
      "access to housing. Throughout, various panelists emphasized that these technologies are being used to shift the \n",
      "burden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in \n",
      "ways that diminish and encroach on equality of opportunity; assessment of these technologies should include \n",
      "whether they are genuinely helpful in solving an identified problem. \n",
      "In discussion of technical and governance interventions that that are needed to protect against the harms of \n",
      "these technologies, panelists individually described the importance of: receiving community input into the \n",
      "design and use of technologies, public reporting on crucial elements of these systems, better notice and consent \n",
      "procedures that ensure privacy based on context and use case, ability to opt-out of using these systems and \n",
      "receive a fallback to a human process, providing explanations of decisions and how these systems work, the \n",
      "need for governance including training in using these systems, ensuring the technological use cases are \n",
      "genuinely related to the goal task and are locally validated to work, and the need for institution and protection \n",
      "of third party audits to ensure systems continue to be accountable and valid. \n",
      "57\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 46, '_id': '003728bdcde746519d32b7f78927b43c', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "SECTION TITLE\n",
      "APPENDIX\n",
      "Listening to the American People \n",
      "The White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill \n",
      "input from people across the country – from impacted communities to industry stakeholders to \n",
      "technology developers to other experts across fields and sectors, as well as policymakers across the Federal \n",
      "government – on the issue of algorithmic and data-driven harms and potential remedies. Through panel \n",
      "discussions, public listening sessions, private meetings, a formal request for information, and input to a \n",
      "publicly accessible and widely-publicized email address, people across the United States spoke up about \n",
      "both the promises and potential harms of these technologies, and played a central role in shaping the \n",
      "Blueprint for an AI Bill of Rights. \n",
      "Panel Discussions to Inform the Blueprint for An AI Bill of Rights \n",
      "OSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress, \n",
      "the Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic \n",
      "Privacy Information Center, and the Mozilla Foundation. The purpose of these convenings – recordings of \n",
      "which are publicly available online112 – was to bring together a variety of experts, practitioners, advocates \n",
      "and federal government officials to offer insights and analysis on the risks, harms, benefits, and \n",
      "policy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \n",
      "theme, exploring current challenges and concerns and considering what an automated society that \n",
      "respects democratic values should look like. These discussions focused on the topics of consumer \n",
      "rights and protections, the criminal justice system, equal opportunities and civil justice, artificial \n",
      "intelligence and democratic values, social welfare and development, and the healthcare system. \n",
      "Summaries of Panel Discussions: \n",
      "Panel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for \n",
      "individual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \n",
      "products, advanced platforms and services, “Internet of Things” (IoT) devices, and smart city products and \n",
      "services. \n",
      "Welcome:\n",
      "•\n",
      "Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "Karen Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German\n",
      "Marshall Fund\n",
      "Moderator: \n",
      "Devin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal \n",
      "Trade Commission \n",
      "Panelists: \n",
      "•\n",
      "Tamika L. Butler, Principal, Tamika L. Butler Consulting\n",
      "•\n",
      "Jennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\n",
      "State University\n",
      "•\n",
      "Carl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\n",
      "•\n",
      "Surya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\n",
      "•\n",
      "Mariah Montgomery, National Campaign Director, Partnership for Working Families\n",
      "55\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 44, '_id': '2ad3dc6e67bc4b25a7016b01de304e06', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "APPENDIX\n",
      "Panel 4: Artificial Intelligence and Democratic Values. This event examined challenges and opportunities in \n",
      "the design of technology that can help support a democratic vision for AI. It included discussion of the \n",
      "technical aspects \n",
      "of \n",
      "designing \n",
      "non-discriminatory \n",
      "technology, \n",
      "explainable \n",
      "AI, \n",
      "human-computer \n",
      "interaction with an emphasis on community participation, and privacy-aware design. \n",
      "Welcome:\n",
      "•\n",
      "Sorelle Friedler, Assistant Director for Data and Democracy, White House Office of Science and\n",
      "Technology Policy\n",
      "•\n",
      "J. Bob Alotta, Vice President for Global Programs, Mozilla Foundation\n",
      "•\n",
      "Navrina Singh, Board Member, Mozilla Foundation\n",
      "Moderator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S \n",
      "Federal Trade Commission. \n",
      "Panelists: \n",
      "•\n",
      "Liz O’Sullivan, CEO, Parity AI\n",
      "•\n",
      "Timnit Gebru, Independent Scholar\n",
      "•\n",
      "Jennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City\n",
      "•\n",
      "Pamela Wisniewski, Associate Professor of Computer Science, University of Central Florida; Director,\n",
      "Socio-technical Interaction Research (STIR) Lab\n",
      "•\n",
      "Seny Kamara, Associate Professor of Computer Science, Brown University\n",
      "Each panelist individually emphasized the risks of using AI in high-stakes settings, including the potential for \n",
      "biased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and \n",
      "understanding of the algorithmic systems. The interventions and key needs various panelists put forward as \n",
      "necessary to the future design of critical AI systems included ongoing transparency, value sensitive and \n",
      "participatory design, explanations designed for relevant stakeholders, and public consultation. \n",
      "Various \n",
      "panelists emphasized the importance of placing trust in people, not technologies, and in engaging with \n",
      "impacted communities to understand the potential harms of technologies and build protection by design into \n",
      "future systems. \n",
      "Panel 5: Social Welfare and Development. This event explored current and emerging uses of technology to \n",
      "implement or improve social welfare systems, social development programs, and other systems that can impact \n",
      "life chances. \n",
      "Welcome:\n",
      "•\n",
      "Suresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\n",
      "and Technology Policy\n",
      "•\n",
      "Anne-Marie Slaughter, CEO, New America\n",
      "Moderator: Michele Evermore, Deputy Director for Policy, Office of Unemployment Insurance \n",
      "Modernization, Office of the Secretary, Department of Labor \n",
      "Panelists:\n",
      "•\n",
      "Blake Hall, CEO and Founder, ID.Me\n",
      "•\n",
      "Karrie Karahalios, Professor of Computer Science, University of Illinois, Urbana-Champaign\n",
      "•\n",
      "Christiaan van Veen, Director of Digital Welfare State and Human Rights Project, NYU School of Law's\n",
      "Center for Human Rights and Global Justice\n",
      "58\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 47, '_id': '580a8fbc4c5149d29b460cfa671195bb', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "59 \n",
      "Tirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n",
      "139-162. https://www.jstor.org/stable/26529441  \n",
      "Tufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \n",
      "Computational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\n",
      "content/uploads/2015/08/Tufekci-ﬁnal.pdf \n",
      "Turri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \n",
      "Practices. AAAI/ACM Conference on AI, Ethics, and Society. \n",
      "https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \n",
      "Urbina, F. et al. (2022) Dual use of artiﬁcial-intelligence-powered drug discovery. Nature Machine \n",
      "Intelligence. https://www.nature.com/articles/s42256-022-00465-9 \n",
      "Wang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \n",
      "https://aclanthology.org/2023.ﬁndings-emnlp.607.pdf \n",
      "Wang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \n",
      "https://arxiv.org/pdf/2308.13387 \n",
      "Wardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \n",
      "policy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\n",
      "framework-for-researc/168076277c \n",
      "Weatherbed, J. (2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge. \n",
      "https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \n",
      "Wei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \n",
      "https://arxiv.org/pdf/2403.18802 \n",
      "Weidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \n",
      "https://arxiv.org/pdf/2112.04359 \n",
      "Weidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \n",
      "https://arxiv.org/pdf/2310.11986 \n",
      "Weidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \n",
      "https://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \n",
      "West, D. (2023) AI poses disproportionate risks to women. Brookings. \n",
      "https://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \n",
      "Wu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \n",
      "analyses. arXiv. https://arxiv.org/pdf/2402.02008 \n",
      "Yin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \n",
      "https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \n",
      "Yu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \n",
      "Language Models. arXiv. https://arxiv.org/html/2403.17336v1 \n",
      "Zaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \n",
      "https://policyreview.info/pdf/policyreview-2022-2-1654.pdf\n",
      "{'source': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'document_id': '2b9b4525-a057-4952-9829-1c50afd5ecd2', 'chunk_number': 52, '_id': '68f70e1000fb4935b90cc6598e2255eb', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "ENDNOTES\n",
      "35. Carrie Johnson. Flaws plague a tool meant to help low-risk federal prisoners win early release. NPR.\n",
      "Jan. 26, 2022. https://www.npr.org/2022/01/26/1075509175/flaws-plague-a-tool-meant-to-help-low­\n",
      "risk-federal-prisoners-win-early-release.; Carrie Johnson. Justice Department works to curb racial bias\n",
      "in deciding who's released from prison. NPR. Apr. 19, 2022. https://\n",
      "www.npr.org/2022/04/19/1093538706/justice-department-works-to-curb-racial-bias-in-deciding­\n",
      "whos-released-from-pris; National Institute of Justice. 2021 Review and Revalidation of the First Step Act\n",
      "Risk Assessment Tool. National Institute of Justice NCJ 303859. Dec., 2021. https://www.ojp.gov/\n",
      "pdffiles1/nij/303859.pdf\n",
      "36. Andrew Thompson. Google’s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://\n",
      "www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias\n",
      "37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of\n",
      "conversations. 2019. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n",
      "38. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and\n",
      "Mitigating Unintended Bias in Text Classification. Proceedings of AAAI/ACM Conference on AI, Ethics,\n",
      "and Society. Feb. 2-3, 2018. https://dl.acm.org/doi/pdf/10.1145/3278721.3278729\n",
      "39. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n",
      "2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina­\n",
      "teenager-2022-03-30/\n",
      "40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.\n",
      "Feb. 2018. https://nyupress.org/9781479837243/algorithms-of-oppression/\n",
      "41. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n",
      "2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina­\n",
      "teenager-2022-03-30/\n",
      "42. Miranda Bogen. All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. May\n",
      "6, 2019. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias\n",
      "43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil\n",
      "Liberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making­\n",
      "flying-easier-for-transgender-people\n",
      "44. U.S. Transportation Security Administration. Transgender/ Non Binary / Gender Nonconforming\n",
      "Passengers. TSA. Accessed Apr. 21, 2022. https://www.tsa.gov/transgender-passengers\n",
      "45. See, e.g., National Disabled Law Students Association. Report on Concerns Regarding Online\n",
      "Administration of Bar Exams. Jul. 29, 2020. https://ndlsa.org/wp-content/uploads/2020/08/\n",
      "NDLSA_Online-Exam-Concerns-Report1.pdf; Lydia X. Z. Brown. How Automated Test Proctoring\n",
      "Software Discriminates Against Disabled Students. Center for Democracy and Technology. Nov. 16, 2020.\n",
      "https://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled­\n",
      "students/\n",
      "46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of\n",
      "populations, 366 Science (2019), https://www.science.org/doi/10.1126/science.aax2342.\n",
      "66\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 54, '_id': '4e581f7cd8f34742aca254935d65b3a4', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\n",
      "Appears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\n",
      "www.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles­\n",
      "in-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,\n",
      "Nov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\n",
      "66. Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.\n",
      "Sept. 24, 2019.\n",
      "https://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\n",
      "67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust\n",
      "Unions. Newsweek. Dec. 13, 2021.\n",
      "https://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust­\n",
      "unions-1658603\n",
      "68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\n",
      "(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\n",
      "against Weight Watchers and their subsidiary Kurbo\n",
      "(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\n",
      "69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\n",
      "(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online\n",
      "Privacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection and\n",
      "Statistical Efficiency Act (CIPSEA) (116 Stat. 2899)\n",
      "70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\n",
      "21, 2018.\n",
      "https://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\n",
      "71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\n",
      "https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n",
      "72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\n",
      "Schools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\n",
      "https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology­\n",
      "schools-are-using-to-monitor-students/\n",
      "73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\n",
      "fighting back. Washington Post. Nov. 12, 2020.\n",
      "https://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\n",
      "74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\n",
      "Technology. May 24, 2022.\n",
      "https://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\n",
      "Lydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\n",
      "Discrimination In New Surveillance Technologies: How new surveillance technologies in education,\n",
      "policing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\n",
      "and Technology Report. May 24, 2022.\n",
      "https://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how­\n",
      "new-surveillance-technologies-in-education-policing-health-care-and-the-workplace­\n",
      "disproportionately-harm-disabled-people/\n",
      "69\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 57, '_id': '2d56d58428d6428db38fef845246b6e2', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n",
      "ENDNOTES\n",
      "1.The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the\n",
      "Federal Government. https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive\n",
      "order-advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\n",
      "2. The White House. Remarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade. Jun.\n",
      "24, 2022. https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/06/24/remarks-by-president­\n",
      "biden-on-the-supreme-court-decision-to-overturn-roe-v-wade/\n",
      "3. The White House. Join the Effort to Create A Bill of Rights for an Automated Society. Nov. 10, 2021. https://\n",
      "www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an­\n",
      "automated-society/\n",
      "4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec’y’s Advisory Comm. on Automated Pers. Data Sys.,\n",
      "Records, Computers, and the Rights of Citizens (July 1973). https://www.justice.gov/opcl/docs/rec-com­\n",
      "rights.pdf.\n",
      "5. See, e.g., Office of Mgmt. & Budget, Exec. Office of the President, Circular A-130, Managing Information as a\n",
      "Strategic Resource, app. II § 3 (July 28, 2016); Org. of Econ. Co-Operation & Dev., Revision of the\n",
      "Recommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder\n",
      "Flows of Personal Data, Annex Part Two (June 20, 2013). https://one.oecd.org/document/C(2013)79/en/pdf.\n",
      "6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in\n",
      "hospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626\n",
      "7. Jessica Guynn. Facebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate\n",
      "speech. USA Today. Apr. 24, 2019. https://www.usatoday.com/story/news/2019/04/24/facebook-while-black­\n",
      "zucked-users-say-they-get-blocked-racism-discussion/2859593002/\n",
      "8. See, e.g., Michael Levitt. AirTags are being used to track people and cars. Here's what is being done about it.\n",
      "NPR. Feb. 18, 2022. https://www.npr.org/2022/02/18/1080944193/apple-airtags-theft-stalking-privacy-tech;\n",
      "Samantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.\n",
      "Motherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking­\n",
      "harassment\n",
      "9. Kristian Lum and William Isaac. To Predict and Serve? Significance. Vol. 13, No. 5, p. 14-19. Oct. 7, 2016.\n",
      "https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Aaron Sankin, Dhruv Mehrotra,\n",
      "Surya Mattu, and Annie Gilbertson. Crime Prediction Software Promised to Be Free of Biases. New Data Shows\n",
      "It Perpetuates Them. The Markup and Gizmodo. Dec. 2, 2021. https://themarkup.org/prediction­\n",
      "bias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates­\n",
      "them\n",
      "10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.\n",
      "June 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman\n",
      "11. Lauren Kaori Gurley. Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make.\n",
      "Motherboard. Sep. 20, 2021. https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing­\n",
      "drivers-for-mistakes-they-didnt-make\n",
      "63\n",
      "{'source': 'Blueprint for an AI Bill of Rights', 'document_id': 'dc1673fc-b57c-45a1-95f1-4656643abd75', 'chunk_number': 51, '_id': 'cd4d80a446034867a5142fb1746a1d26', '_collection_name': '1cbc02a9b58744019c07e985ed628304'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me about Karen Hao\"\n",
    "results = model_1000_100_state.retriever.get_relevant_documents(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - that all looks good - lets continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RAG Chain for the different models\n",
    "\n",
    "This will take a model_run_state that will pass in:\n",
    "- the qa model\n",
    "- the retriever \n",
    "\n",
    "It creates the RAG chain and saves it in the model_run_state for RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Created\n"
     ]
    }
   ],
   "source": [
    "from utilities.templates import get_qa_prompt\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from utilities.debugger import dprint\n",
    "\n",
    "def create_rag_chain(app_state, model_run_state):\n",
    "\n",
    "    chat_prompt = get_qa_prompt()\n",
    "\n",
    "    simple_chain = chat_prompt | model_run_state.qa_model\n",
    "    dprint(app_state, simple_chain.invoke({\"question\": \"Can you give me a summary of the 2 documents\", \"context\":\"\"}))\n",
    "\n",
    "    rag_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | model_run_state.retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "\n",
    "\n",
    "        | {\"response\": chat_prompt | model_run_state.qa_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    response = rag_qa_chain.invoke({\"question\" : \"What is the AI Bill of Rights \"})\n",
    "    dprint(app_state, response)\n",
    "    dprint(app_state, response[\"response\"].content)\n",
    "    dprint(app_state, f\"Number of found context: {len(response['context'])}\")\n",
    "    model_run_state.rag_qa_chain = rag_qa_chain\n",
    "    print(\"RAG Chain Created\")\n",
    "\n",
    "create_rag_chain(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG - Create the questions for evaluation\n",
    "\n",
    "3 functions are used to set up the questions:\n",
    "- batch_chunks - processes batches of chunks to try and get past the limitations with OpenAI quotas\n",
    "- create_chunks_for_ragas - takes the documents and splits them up based on the RagasState - this will allow us more experimentation later\n",
    "- create_questions_for_ragas - sets up the generator and creates the number of questions and distribution based on RagasState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.ragas_state import RagasState\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "import time\n",
    "\n",
    "# create document chunks\n",
    "def Create_chunks_for_ragas(app_state, ragas_state):\n",
    "    # we have 2 documents so want representative across both\n",
    "    text_splitter_eval = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = ragas_state.chunk_size,\n",
    "        chunk_overlap = ragas_state.chunk_overlap,\n",
    "        length_function = len\n",
    "    )\n",
    "    combined_chunks_document = []\n",
    "    for document in app_state.documents:\n",
    "        eval_document = document[\"loaded_document\"]\n",
    "        document_chunks = text_splitter_eval.split_documents(eval_document)\n",
    "        print(f\"Num chumks: {len(document_chunks)}\")\n",
    "        combined_chunks_document = combined_chunks_document + document_chunks\n",
    "\n",
    "    print(f\"Total chunks: {len(combined_chunks_document)}\")\n",
    "    ragas_state.chunks = combined_chunks_document\n",
    "    print()\n",
    "\n",
    "# submit batches\n",
    "def batch_chunks(chunks, batch_size):\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        yield chunks[i:i + batch_size]\n",
    "\n",
    "# create the questions\n",
    "def create_questions_for_ragas(app_state, ragas_state):\n",
    "    generator_llm = ChatOpenAI(model=ragas_state.generator_llm)\n",
    "    critic_llm = ChatOpenAI(model=ragas_state.critic_llm)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    generator = TestsetGenerator.from_langchain(\n",
    "        generator_llm,\n",
    "        critic_llm,\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    batch_size = 200  # Number of chunks to process per batch\n",
    "    delay_between_batches = 1  # 1 second delay between batches\n",
    "\n",
    "    all_test_data = []\n",
    "\n",
    "    for batch in batch_chunks(ragas_state.chunks, batch_size):\n",
    "        print(f\"Processing batch of {len(batch)} chunks...\")\n",
    "\n",
    "        # Generate testset for the current batch\n",
    "        testset = generator.generate_with_langchain_docs(\n",
    "            batch,  # Process this batch of chunks\n",
    "            ragas_state.num_questions, \n",
    "            ragas_state.distributions\n",
    "        )\n",
    "\n",
    "        # Convert the testset to pandas and store the result\n",
    "        testset_df = testset.to_pandas()\n",
    "        all_test_data.append(testset_df)\n",
    "\n",
    "        # Wait 1 second before the next batch\n",
    "        time.sleep(delay_between_batches)\n",
    "\n",
    "\n",
    "\n",
    "    combined_testset_df = pd.concat(all_test_data, ignore_index=True)\n",
    "    ragas_state.testset_df = combined_testset_df\n",
    "\n",
    "    print(\"Ragas questions created for all batches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ragas_state if it exists offline \n",
    "\n",
    "Otherwise we will need to run the question creation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# File path where ragas_state is stored\n",
    "file_path = 'ragas_state.pkl'\n",
    "\n",
    "# Check if the pickled file exists, and load it if it does\n",
    "def load_ragas_state_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                ragas_state = pickle.load(f)\n",
    "            print(f\"Ragas state loaded from {file_path}\")\n",
    "            return ragas_state\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ragas state: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No existing ragas state found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Use this to load ragas_state from pickle\n",
    "# ragas_state = load_ragas_state_if_exists(file_path)\n",
    "# ragas_state.distributions = {\n",
    "#             simple: 0.5,\n",
    "#             multi_context: 0.4,\n",
    "#             reasoning: 0.1\n",
    "#         }\n",
    "\n",
    "# use this to rebuild pickle state\n",
    "# ragas_state = RagasState()\n",
    "# ragas_state.generator_llm = \"gpt-4o\"\n",
    "# Create_chunks_for_ragas(app_state, ragas_state)\n",
    "# create_questions_for_ragas(app_state, ragas_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in testset_df: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of entries in testset_df: {ragas_state.testset_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save off the ragas state - then comment out above code so we don't need to run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas state loaded from ragas_state.pkl\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the ragas_state without distributions\n",
    "def save_ragas_state(ragas_state, file_path):\n",
    "    # Temporarily remove `distributions` before saving\n",
    "    distributions_backup = ragas_state.distributions\n",
    "    ragas_state.distributions = None\n",
    "\n",
    "    # Save the rest of the object\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(ragas_state, f)\n",
    "        print(f\"Ragas state saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving ragas state: {e}\")\n",
    "    \n",
    "    # Restore `distributions` after saving\n",
    "    ragas_state.distributions = distributions_backup\n",
    "\n",
    "# Uncomment and run if need to save new ragas_state\n",
    "# save_ragas_state(ragas_state, 'ragas_state.pkl')\n",
    "\n",
    "# lets see if unpickle works\n",
    "# test_ragas_state = load_ragas_state_if_exists(file_path)\n",
    "# print(len(test_ragas_state.testset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers based on the pipeline we have created\n",
    "\n",
    "This function takes in the model_run_state and the ragas_state and uses the retriever from the model_run_state to answer the questions from the ragas_state. \n",
    "\n",
    "The response dataset is stored in the model_run_state for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers created - ready for Ragas evaluation\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def create_answers(app_state, model_run_state, ragas_state):\n",
    "  answers = []\n",
    "  contexts = []\n",
    "\n",
    "  test_questions = ragas_state.testset_df[\"question\"].values.tolist()\n",
    "  test_groundtruths = ragas_state.testset_df[\"ground_truth\"].values.tolist()\n",
    "\n",
    "  for question in test_questions:\n",
    "    response = model_run_state.rag_qa_chain.invoke({\"question\" : question})\n",
    "    answers.append(response[\"response\"].content)\n",
    "    contexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "  # Wrap it in a huggingface dataset\n",
    "  model_run_state.response_dataset = Dataset.from_dict({\n",
    "      \"question\" : test_questions,\n",
    "      \"answer\" : answers,\n",
    "      \"contexts\" : contexts,\n",
    "      \"ground_truth\" : test_groundtruths\n",
    "  })\n",
    "  model_run_state.response_dataset[0]\n",
    "  print(\"Answers created - ready for Ragas evaluation\")\n",
    "\n",
    "create_answers(app_state, model_1000_100_state, ragas_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The run_ragas_evaluation uses the response dataset from the previous step stored in the model_run_state to determine the requested Ragas metrics.\n",
    "\n",
    "The results of the evaluation are then stored nack in the model_run_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|█████▊    | 58/100 [02:14<03:09,  4.50s/it]Exception raised in Job[27]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 195927, Requested 11789. Please try again in 2.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  69%|██████▉   | 69/100 [02:36<01:18,  2.52s/it]Exception raised in Job[40]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 199471, Requested 11292. Please try again in 3.228s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  79%|███████▉  | 79/100 [03:02<00:50,  2.41s/it]Exception raised in Job[85]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 189772, Requested 11284. Please try again in 316ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  81%|████████  | 81/100 [03:10<01:06,  3.50s/it]Exception raised in Job[25]: TimeoutError()\n",
      "Evaluating:  82%|████████▏ | 82/100 [03:10<00:44,  2.48s/it]Exception raised in Job[26]: TimeoutError()\n",
      "Evaluating:  90%|█████████ | 90/100 [03:29<00:18,  1.85s/it]Exception raised in Job[61]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 100/100 [04:16<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "def run_ragas_evaluation(app_state, model_run_state):\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_correctness,\n",
    "    ]\n",
    "    model_run_state.ragas_results = evaluate(model_run_state.response_dataset, metrics)\n",
    "    print(\"Ragas evaluation complete\")\n",
    "run_ragas_evaluation(app_state, model_1000_100_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Model\n",
    "\n",
    "We can report on the model and the paramters such as the chunking size, overlap, and the Ragas metrics both summary and per question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: text-embedding-3-small\n",
      "Chunk size: 1000\n",
      "Chunk overlap: 100\n",
      "{'faithfulness': 0.8230, 'answer_relevancy': 0.7518, 'context_recall': 0.9474, 'context_precision': 0.7225, 'answer_correctness': 0.4931}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How might an insurer use social media data col...</td>\n",
       "      <td>It appears that insurers might use social medi...</td>\n",
       "      <td>[DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...</td>\n",
       "      <td>An insurer might collect data from a person's ...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.578716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the potential impacts of non-consensu...</td>\n",
       "      <td>The potential impacts of non-consensual intima...</td>\n",
       "      <td>[11 \\nvalue chain (e.g., data inputs, processi...</td>\n",
       "      <td>The experience of harm to victims of non-conse...</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.973334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.729688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the 3 AI biases tied to dataset, test...</td>\n",
       "      <td>The three AI biases tied to dataset, testing, ...</td>\n",
       "      <td>[57 \\nNational Institute of Standards and Tech...</td>\n",
       "      <td>The three categories of bias in AI are systemi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.227270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can creators of automated systems ensure l...</td>\n",
       "      <td>Creators of automated systems can ensure legal...</td>\n",
       "      <td>[WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962654</td>\n",
       "      <td>0.387657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the AI Bill of Rights guide responsib...</td>\n",
       "      <td>The AI Bill of Rights guides the responsible u...</td>\n",
       "      <td>[-    \\nUSING THIS TECHNICAL COMPANION\\nThe Bl...</td>\n",
       "      <td>The AI Bill of Rights guides the responsible u...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is data collection emphasized as a necessa...</td>\n",
       "      <td>Data collection is emphasized as a necessary i...</td>\n",
       "      <td>[You should be protected from abusive data pra...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are some examples of protected classifica...</td>\n",
       "      <td>Some examples of protected classifications tha...</td>\n",
       "      <td>[WHY THIS PRINCIPLE IS IMPORTANT\\nThis section...</td>\n",
       "      <td>Some examples of protected classifications tha...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.382027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do hybrid AI-human platforms balance effic...</td>\n",
       "      <td>The balance between efficiency and fair human ...</td>\n",
       "      <td>[HUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFA...</td>\n",
       "      <td>Hybrid AI-human platforms balance efficiency a...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928263</td>\n",
       "      <td>0.847716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What risks come from unregulated consumer data...</td>\n",
       "      <td>The risks from unregulated consumer data colle...</td>\n",
       "      <td>[DATA PRIVACY \\nWHY THIS PRINCIPLE IS IMPORTAN...</td>\n",
       "      <td>The risks from unregulated consumer data colle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.931796</td>\n",
       "      <td>0.754445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How can legal discovery balance oversight and ...</td>\n",
       "      <td>The documents provided do not specifically add...</td>\n",
       "      <td>[NOTICE &amp; \\nEXPLANATION \\nHOW THESE PRINCIPLES...</td>\n",
       "      <td>Legal discovery can balance oversight and prot...</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.653704</td>\n",
       "      <td>0.355803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What information is required under the \"System...</td>\n",
       "      <td>I don't have enough information, sorry.</td>\n",
       "      <td>[45 \\nMG-4.1-007 \\nVerify that AI Actors respo...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do text-to-image models perpetuate and amp...</td>\n",
       "      <td>Text-to-image models can perpetuate and amplif...</td>\n",
       "      <td>[8 \\nTrustworthy AI Characteristics: Accountab...</td>\n",
       "      <td>Text-to-image models perpetuate and amplify ha...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What must lenders disclose if terms are less f...</td>\n",
       "      <td>Lenders must disclose if terms are less favora...</td>\n",
       "      <td>[NOTICE &amp; \\nEXPLANATION \\nHOW THESE PRINCIPLES...</td>\n",
       "      <td>Lenders must inform consumers when they are ge...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.634663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How do humans ensure automated systems' reliab...</td>\n",
       "      <td>In aviation security and elections, ensuring t...</td>\n",
       "      <td>[HUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFA...</td>\n",
       "      <td>Humans ensure the reliability of automated sys...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976543</td>\n",
       "      <td>0.551571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How should ethically sensitive contexts be add...</td>\n",
       "      <td>To address ethically sensitive contexts in the...</td>\n",
       "      <td>[48 \\n• Data protection \\n• Data retention  \\n...</td>\n",
       "      <td>Ethically sensitive contexts should be address...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What privacy risks are associated with data me...</td>\n",
       "      <td>Data memorization in large language models (LL...</td>\n",
       "      <td>[DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...</td>\n",
       "      <td>Data memorization in LLMs may pose exacerbated...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How can AI red-teaming help in identifying pot...</td>\n",
       "      <td>AI red-teaming is an important practice in ide...</td>\n",
       "      <td>[50 \\nParticipatory Engagement Methods \\nOn an...</td>\n",
       "      <td>AI red-teaming helps in identifying potential ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.898481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962654</td>\n",
       "      <td>0.938788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do tailored standards and safeguards ensur...</td>\n",
       "      <td>Tailored standards and safeguards ensure the s...</td>\n",
       "      <td>[SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE...</td>\n",
       "      <td>Tailored standards and safeguards ensure autom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What staged release strategies can mitigate hi...</td>\n",
       "      <td>It seems like you're asking about strategies t...</td>\n",
       "      <td>[17 \\nGOVERN 1.7: Processes and procedures are...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Which study shows bias in GPT detectors vs. no...</td>\n",
       "      <td>The study that shows bias in GPT detectors aga...</td>\n",
       "      <td>[59 \\nTirrell, L. (2017) Toxic Speech: Toward ...</td>\n",
       "      <td>Liang, W. et al. (2023) GPT detectors are bias...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.537869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   How might an insurer use social media data col...   \n",
       "1   What are the potential impacts of non-consensu...   \n",
       "2   What are the 3 AI biases tied to dataset, test...   \n",
       "3   How can creators of automated systems ensure l...   \n",
       "4   How does the AI Bill of Rights guide responsib...   \n",
       "5   Why is data collection emphasized as a necessa...   \n",
       "6   What are some examples of protected classifica...   \n",
       "7   How do hybrid AI-human platforms balance effic...   \n",
       "8   What risks come from unregulated consumer data...   \n",
       "9   How can legal discovery balance oversight and ...   \n",
       "10  What information is required under the \"System...   \n",
       "11  How do text-to-image models perpetuate and amp...   \n",
       "12  What must lenders disclose if terms are less f...   \n",
       "13  How do humans ensure automated systems' reliab...   \n",
       "14  How should ethically sensitive contexts be add...   \n",
       "15  What privacy risks are associated with data me...   \n",
       "16  How can AI red-teaming help in identifying pot...   \n",
       "17  How do tailored standards and safeguards ensur...   \n",
       "18  What staged release strategies can mitigate hi...   \n",
       "19  Which study shows bias in GPT detectors vs. no...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   It appears that insurers might use social medi...   \n",
       "1   The potential impacts of non-consensual intima...   \n",
       "2   The three AI biases tied to dataset, testing, ...   \n",
       "3   Creators of automated systems can ensure legal...   \n",
       "4   The AI Bill of Rights guides the responsible u...   \n",
       "5   Data collection is emphasized as a necessary i...   \n",
       "6   Some examples of protected classifications tha...   \n",
       "7   The balance between efficiency and fair human ...   \n",
       "8   The risks from unregulated consumer data colle...   \n",
       "9   The documents provided do not specifically add...   \n",
       "10            I don't have enough information, sorry.   \n",
       "11  Text-to-image models can perpetuate and amplif...   \n",
       "12  Lenders must disclose if terms are less favora...   \n",
       "13  In aviation security and elections, ensuring t...   \n",
       "14  To address ethically sensitive contexts in the...   \n",
       "15  Data memorization in large language models (LL...   \n",
       "16  AI red-teaming is an important practice in ide...   \n",
       "17  Tailored standards and safeguards ensure the s...   \n",
       "18  It seems like you're asking about strategies t...   \n",
       "19  The study that shows bias in GPT detectors aga...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...   \n",
       "1   [11 \\nvalue chain (e.g., data inputs, processi...   \n",
       "2   [57 \\nNational Institute of Standards and Tech...   \n",
       "3   [WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\...   \n",
       "4   [-    \\nUSING THIS TECHNICAL COMPANION\\nThe Bl...   \n",
       "5   [You should be protected from abusive data pra...   \n",
       "6   [WHY THIS PRINCIPLE IS IMPORTANT\\nThis section...   \n",
       "7   [HUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFA...   \n",
       "8   [DATA PRIVACY \\nWHY THIS PRINCIPLE IS IMPORTAN...   \n",
       "9   [NOTICE & \\nEXPLANATION \\nHOW THESE PRINCIPLES...   \n",
       "10  [45 \\nMG-4.1-007 \\nVerify that AI Actors respo...   \n",
       "11  [8 \\nTrustworthy AI Characteristics: Accountab...   \n",
       "12  [NOTICE & \\nEXPLANATION \\nHOW THESE PRINCIPLES...   \n",
       "13  [HUMAN ALTERNATIVES, \\nCONSIDERATION, AND \\nFA...   \n",
       "14  [48 \\n• Data protection \\n• Data retention  \\n...   \n",
       "15  [DATA PRIVACY \\nEXTRA PROTECTIONS FOR DATA REL...   \n",
       "16  [50 \\nParticipatory Engagement Methods \\nOn an...   \n",
       "17  [SAFE AND EFFECTIVE \\nSYSTEMS \\nWHAT SHOULD BE...   \n",
       "18  [17 \\nGOVERN 1.7: Processes and procedures are...   \n",
       "19  [59 \\nTirrell, L. (2017) Toxic Speech: Toward ...   \n",
       "\n",
       "                                         ground_truth  faithfulness  \\\n",
       "0   An insurer might collect data from a person's ...      0.875000   \n",
       "1   The experience of harm to victims of non-conse...      0.933333   \n",
       "2   The three categories of bias in AI are systemi...      1.000000   \n",
       "3   Designers, developers, and deployers of automa...      1.000000   \n",
       "4   The AI Bill of Rights guides the responsible u...      1.000000   \n",
       "5   The answer to given question is not present in...           NaN   \n",
       "6   Some examples of protected classifications tha...      1.000000   \n",
       "7   Hybrid AI-human platforms balance efficiency a...      1.000000   \n",
       "8   The risks from unregulated consumer data colle...           NaN   \n",
       "9   Legal discovery can balance oversight and prot...      0.533333   \n",
       "10  The answer to given question is not present in...      0.000000   \n",
       "11  Text-to-image models perpetuate and amplify ha...      1.000000   \n",
       "12  Lenders must inform consumers when they are ge...      0.750000   \n",
       "13  Humans ensure the reliability of automated sys...      1.000000   \n",
       "14  Ethically sensitive contexts should be address...      1.000000   \n",
       "15  Data memorization in LLMs may pose exacerbated...      1.000000   \n",
       "16  AI red-teaming helps in identifying potential ...      1.000000   \n",
       "17  Tailored standards and safeguards ensure autom...           NaN   \n",
       "18  The answer to given question is not present in...      0.900000   \n",
       "19  Liang, W. et al. (2023) GPT detectors are bias...      0.000000   \n",
       "\n",
       "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
       "0           0.000000             1.0           0.500000            0.578716  \n",
       "1           0.973334             1.0           0.825397            0.729688  \n",
       "2           0.997365             1.0           0.500000            0.227270  \n",
       "3           0.981650             1.0           0.962654            0.387657  \n",
       "4           0.954697             1.0           1.000000            0.462809  \n",
       "5                NaN             NaN           0.000000            0.181015  \n",
       "6           0.992633             1.0           0.916667            0.382027  \n",
       "7           0.964648             1.0           0.928263            0.847716  \n",
       "8           0.985682             1.0           0.931796            0.754445  \n",
       "9           0.000000             0.0           0.653704            0.355803  \n",
       "10          0.000000             1.0           0.000000            0.197128  \n",
       "11          1.000000             1.0           1.000000            0.933494  \n",
       "12               NaN             1.0           0.650000            0.634663  \n",
       "13          0.959559             1.0           0.976543            0.551571  \n",
       "14          0.938381             1.0           1.000000            0.386925  \n",
       "15          0.930288             1.0           1.000000            0.309642  \n",
       "16          0.898481             1.0           0.962654            0.938788  \n",
       "17          0.978326             1.0           1.000000            0.284306  \n",
       "18          0.000000             1.0           0.000000            0.181160  \n",
       "19          0.976875             1.0           0.642857            0.537869  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1000_100_state.parameters()\n",
    "#model_1000_100_state.results_summary()\n",
    "model_1000_100_state.results()\n",
    "\n",
    "print(model_1000_100_state.ragas_results)\n",
    "results_df = model_1000_100_state.ragas_results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Snowflake Model Evaluation\n",
    "\n",
    "Process:\n",
    "- set up the model_run_state for the base Snowflake model\n",
    "- create the vector store using the base model\n",
    "- create the RAG chain with the retriever for the vestor store\n",
    "- generate the answers to the Ragas questions\n",
    "- evaluate the model's performance using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-360' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29970, Requested 406. Please try again in 752ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29970, Requested 406. Please try again in 752ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-396' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29962, Requested 406. Please try again in 736ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29962, Requested 406. Please try again in 736ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-229' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29942, Requested 403. Please try again in 690ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29942, Requested 403. Please try again in 690ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-348' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29599, Requested 419. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29599, Requested 419. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-211' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29944, Requested 409. Please try again in 706ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29944, Requested 409. Please try again in 706ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-468' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29936, Requested 411. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29936, Requested 411. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-505' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29947, Requested 410. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29947, Requested 410. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-487' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29960, Requested 413. Please try again in 746ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29960, Requested 413. Please try again in 746ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1166' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29954, Requested 408. Please try again in 724ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29954, Requested 408. Please try again in 724ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-984' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29925, Requested 415. Please try again in 680ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29925, Requested 415. Please try again in 680ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-786' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29926, Requested 412. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29926, Requested 412. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-990' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29953, Requested 414. Please try again in 734ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29953, Requested 414. Please try again in 734ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-787' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29957, Requested 417. Please try again in 748ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29957, Requested 417. Please try again in 748ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-803' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29938, Requested 400. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29938, Requested 400. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1026' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29954, Requested 411. Please try again in 730ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29954, Requested 411. Please try again in 730ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-637' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29867, Requested 414. Please try again in 562ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29867, Requested 414. Please try again in 562ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-643' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29943, Requested 404. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29943, Requested 404. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-857' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29915, Requested 408. Please try again in 646ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29915, Requested 408. Please try again in 646ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-881' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29939, Requested 407. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29939, Requested 407. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-930' coro=<as_completed.<locals>.sema_coro() done, defined at /home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py:32> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29973, Requested 406. Please try again in 758ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 60, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/executor.py\", line 54, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/ragas/llms/base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 724, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 684, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 883, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rchrdgwr/anaconda3/envs/clean-llmops/lib/python3.11/site-packages/openai/_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 30000, Used 29973, Requested 406. Please try again in 758ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "RAG Chain Created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  59%|█████▉    | 59/100 [01:55<01:24,  2.05s/it]Exception raised in Job[46]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 193304, Requested 7041. Please try again in 103ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  67%|██████▋   | 67/100 [02:14<01:04,  1.94s/it]Exception raised in Job[85]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 195043, Requested 8408. Please try again in 1.035s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  79%|███████▉  | 79/100 [02:47<00:46,  2.22s/it]Exception raised in Job[52]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 195041, Requested 9383. Please try again in 1.327s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  90%|█████████ | 90/100 [03:23<00:28,  2.88s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Evaluating:  93%|█████████▎| 93/100 [03:27<00:14,  2.11s/it]Exception raised in Job[26]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 100/100 [04:03<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n"
     ]
    }
   ],
   "source": [
    "snowflake_base_state = ModelRunState()\n",
    "snowflake_base_state.name = \"Snowflake_Base/1000/100\"\n",
    "snowflake_base_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_base_state.qa_model = ChatOpenAI(model=snowflake_base_state.qa_model_name)\n",
    "\n",
    "# snowflake embedding model\n",
    "snowflake_base_state.embedding_model_name = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "snowflake_base_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_base_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_base_state.chunk_size = 1000\n",
    "snowflake_base_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_base_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_base_state)\n",
    "create_answers(app_state, snowflake_base_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_base_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Snowflake model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: Snowflake/snowflake-arctic-embed-m\n",
      "Chink size: 1000\n",
      "Chink overlap: 100\n",
      "{'faithfulness': 0.4478, 'answer_relevancy': 0.6039, 'context_recall': 0.3333, 'context_precision': 0.5833, 'answer_correctness': 0.3635}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can new GAI policies, procedures, and proc...</td>\n",
       "      <td>The connection between new Generative AI (GAI)...</td>\n",
       "      <td>[Table of Contents \\n1. \\nIntroduction ..........</td>\n",
       "      <td>New GAI policies, procedures, and processes ca...</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.892835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.506942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does confirmation bias contribute to poten...</td>\n",
       "      <td>I don't have enough information, sorry. Howeve...</td>\n",
       "      <td>[BLUEPRINT FOR AN \\nAI BILL OF \\nRIGHTS \\nMAKI...</td>\n",
       "      <td>Confirmation bias contributes to potentially i...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What resources on AI risk management are avail...</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>[57 \\nNational Institute of Standards and Tech...</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.404474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How can new GAI policies, procedures, and proc...   \n",
       "1  How does confirmation bias contribute to poten...   \n",
       "2  What resources on AI risk management are avail...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The connection between new Generative AI (GAI)...   \n",
       "1  I don't have enough information, sorry. Howeve...   \n",
       "2  The National Institute of Standards and Techno...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Table of Contents \\n1. \\nIntroduction ..........   \n",
       "1  [BLUEPRINT FOR AN \\nAI BILL OF \\nRIGHTS \\nMAKI...   \n",
       "2  [57 \\nNational Institute of Standards and Tech...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  New GAI policies, procedures, and processes ca...      0.454545   \n",
       "1  Confirmation bias contributes to potentially i...      0.000000   \n",
       "2  The National Institute of Standards and Techno...      0.888889   \n",
       "\n",
       "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
       "0          0.892835             0.0           0.833333            0.506942  \n",
       "1          0.000000             0.0           0.000000            0.179080  \n",
       "2          0.918800             1.0           0.916667            0.404474  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "snowflake_base_state.parameters()\n",
    "print(snowflake_base_state.ragas_results)\n",
    "results_df = snowflake_base_state.ragas_results.to_pandas()\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Base Snowflake with the TE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Snowflake_Base/1000/100</th>\n",
       "      <th>TE3/1000/100</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.447811</td>\n",
       "      <td>0.635913</td>\n",
       "      <td>0.188101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.603878</td>\n",
       "      <td>0.947800</td>\n",
       "      <td>0.343922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.299809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric  Snowflake_Base/1000/100  TE3/1000/100  Difference\n",
       "0        faithfulness                 0.447811      0.635913    0.188101\n",
       "1    answer_relevancy                 0.603878      0.947800    0.343922\n",
       "2      context_recall                 0.333333      0.833333    0.500000\n",
       "3   context_precision                 0.583333      1.000000    0.416667\n",
       "4  answer_correctness                 0.363499      0.663308    0.299809"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def compare_results(run_model_1, run_model_2):\n",
    "    results_1 = run_model_1.ragas_results\n",
    "    results_2 = run_model_2.ragas_results\n",
    "    comparison_data = {\n",
    "        'Metric': list(results_1.keys()),\n",
    "        run_model_1.name: [results_1[key] for key in results_1.keys()],\n",
    "        run_model_2.name: [results_2[key] for key in results_2.keys()],\n",
    "        'Difference': [results_2[key] - results_1[key] for key in results_1.keys()]\n",
    "    }\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "snowflake_base_state.name = \"Snowflake_Base/1000/100\"\n",
    "model_1000_100_state.name = \"TE3/1000/100\"\n",
    "df = compare_results(snowflake_base_state, model_1000_100_state )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:19<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "snowflake_finetune_state = ModelRunState()\n",
    "snowflake_finetune_state.name = \"Snowflake_Fine/1000/100\"\n",
    "snowflake_finetune_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_state.qa_model = ChatOpenAI(model=snowflake_finetune_state.qa_model_name)\n",
    "\n",
    "# finetune snowflake embedding model\n",
    "\n",
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model\"\n",
    "\n",
    "# Load the fine-tuned model from Hugging Face\n",
    "snowflake_finetune_state.embedding_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_state.chunk_size = 1000\n",
    "snowflake_finetune_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_state)\n",
    "create_answers(app_state, snowflake_finetune_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt-4o-mini\n",
      "Embedding model: rchrdgwr/finetuned-arctic-model\n",
      "Chink size: 1000\n",
      "Chink overlap: 100\n",
      "{'faithfulness': 0.9103, 'answer_relevancy': 0.9455, 'context_recall': 0.8889, 'context_precision': 1.0000, 'answer_correctness': 0.4178}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can new GAI policies, procedures, and proc...</td>\n",
       "      <td>New GAI (Generative Artificial Intelligence) p...</td>\n",
       "      <td>[19 \\nGV-4.1-003 \\nEstablish policies, procedu...</td>\n",
       "      <td>New GAI policies, procedures, and processes ca...</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.934013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does confirmation bias contribute to poten...</td>\n",
       "      <td>Confirmation bias can contribute to potentiall...</td>\n",
       "      <td>[Algorithmic \\nDiscrimination \\nProtections \\n...</td>\n",
       "      <td>Confirmation bias contributes to potentially i...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966214</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What resources on AI risk management are avail...</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>[NIST Trustworthy and Responsible AI  \\nNIST A...</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.388704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How can new GAI policies, procedures, and proc...   \n",
       "1  How does confirmation bias contribute to poten...   \n",
       "2  What resources on AI risk management are avail...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  New GAI (Generative Artificial Intelligence) p...   \n",
       "1  Confirmation bias can contribute to potentiall...   \n",
       "2  The National Institute of Standards and Techno...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [19 \\nGV-4.1-003 \\nEstablish policies, procedu...   \n",
       "1  [Algorithmic \\nDiscrimination \\nProtections \\n...   \n",
       "2  [NIST Trustworthy and Responsible AI  \\nNIST A...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  New GAI policies, procedures, and processes ca...      0.730769   \n",
       "1  Confirmation bias contributes to potentially i...      1.000000   \n",
       "2  The National Institute of Standards and Techno...      1.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
       "0          0.934013        1.000000                1.0            0.527957  \n",
       "1          0.966214        0.666667                1.0            0.336860  \n",
       "2          0.936206        1.000000                1.0            0.388704  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowflake_finetune_state.parameters()\n",
    "print(snowflake_finetune_state.ragas_results)\n",
    "results_df = snowflake_finetune_state.ragas_results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of TE3, Base Snowflake, and Fine Tuned Snowflake (1st Model) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>TE3/1000/100</th>\n",
       "      <th>Snowflake_Base/1000/100</th>\n",
       "      <th>Snowflake_Fine/1000/100</th>\n",
       "      <th>1v2 Difference</th>\n",
       "      <th>1v3 Difference</th>\n",
       "      <th>2v3 Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.635913</td>\n",
       "      <td>0.447811</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>-0.188101</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>0.462445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.947800</td>\n",
       "      <td>0.603878</td>\n",
       "      <td>0.945477</td>\n",
       "      <td>-0.343922</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>0.341599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.417840</td>\n",
       "      <td>-0.299809</td>\n",
       "      <td>-0.245467</td>\n",
       "      <td>0.054342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric  TE3/1000/100  Snowflake_Base/1000/100  \\\n",
       "0        faithfulness      0.635913                 0.447811   \n",
       "1    answer_relevancy      0.947800                 0.603878   \n",
       "2      context_recall      0.833333                 0.333333   \n",
       "3   context_precision      1.000000                 0.583333   \n",
       "4  answer_correctness      0.663308                 0.363499   \n",
       "\n",
       "   Snowflake_Fine/1000/100  1v2 Difference  1v3 Difference  2v3 Difference  \n",
       "0                 0.910256       -0.188101        0.274344        0.462445  \n",
       "1                 0.945477       -0.343922       -0.002323        0.341599  \n",
       "2                 0.888889       -0.500000        0.055556        0.555556  \n",
       "3                 1.000000       -0.416667        0.000000        0.416667  \n",
       "4                 0.417840       -0.299809       -0.245467        0.054342  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def compare_results_3(run_model_1, run_model_2, run_model_3):\n",
    "    # Extract results for each model\n",
    "    results_1 = run_model_1.ragas_results\n",
    "    results_2 = run_model_2.ragas_results\n",
    "    results_3 = run_model_3.ragas_results\n",
    "\n",
    "    # Create comparison data\n",
    "    comparison_data = {\n",
    "        'Metric': list(results_1.keys()),\n",
    "        run_model_1.name: [results_1[key] for key in results_1.keys()],\n",
    "        run_model_2.name: [results_2[key] for key in results_2.keys()],\n",
    "        run_model_3.name: [results_3[key] for key in results_3.keys()],\n",
    "        '1v2 Difference': [results_2[key] - results_1[key] for key in results_1.keys()],\n",
    "        '1v3 Difference': [results_3[key] - results_1[key] for key in results_1.keys()],\n",
    "        '2v3 Difference': [results_3[key] - results_2[key] for key in results_2.keys()]\n",
    "    }\n",
    "\n",
    "    # Return the dataframe\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "snowflake_base_state.name = \"Snowflake_Base/1000/100\"\n",
    "model_1000_100_state.name = \"TE3/1000/100\"\n",
    "df = compare_results_3(model_1000_100_state , snowflake_base_state,  snowflake_finetune_state)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model\"\n",
    "\n",
    "snowflake_finetune_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Section Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rchrdgwr/finetuned-arctic-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:13<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n",
      "{'faithfulness': 0.9010, 'answer_relevancy': 0.9697, 'context_recall': 0.8889, 'context_precision': 1.0000, 'answer_correctness': 0.3700}\n"
     ]
    }
   ],
   "source": [
    "from utilities.constants import (\n",
    "    CHUNKING_STRATEGY_TABLE_AWARE,\n",
    "    CHUNKING_STRATEGY_SECTION_BASED,\n",
    "    CHUNKING_STRATEGY_SEMANTIC\n",
    ")\n",
    "\n",
    "snowflake_finetune_section_state = ModelRunState()\n",
    "snowflake_finetune_section_state.name = \"Snowflake_FineSection/1000/100\"\n",
    "snowflake_finetune_section_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_section_state.qa_model = ChatOpenAI(model=snowflake_finetune_section_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_section_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_section_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_section_state.chunking_strategy = CHUNKING_STRATEGY_SECTION_BASED\n",
    "snowflake_finetune_section_state.chunk_size = 1000\n",
    "snowflake_finetune_section_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_section_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_section_state)\n",
    "create_answers(app_state, snowflake_finetune_section_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_section_state)\n",
    "print(snowflake_finetune_section_state.ragas_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Table Aware Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:17<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n",
      "{'faithfulness': 0.6922, 'answer_relevancy': 0.9457, 'context_recall': 0.8889, 'context_precision': 1.0000, 'answer_correctness': 0.4848}\n"
     ]
    }
   ],
   "source": [
    "snowflake_finetune_table_state = ModelRunState()\n",
    "snowflake_finetune_table_state.name = \"Snowflake_FineTable/1000/100\"\n",
    "snowflake_finetune_table_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_table_state.qa_model = ChatOpenAI(model=snowflake_finetune_table_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_table_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_table_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_table_state.chunking_strategy = CHUNKING_STRATEGY_TABLE_AWARE\n",
    "snowflake_finetune_table_state.chunk_size = 1000\n",
    "snowflake_finetune_table_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_table_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_table_state)\n",
    "create_answers(app_state, snowflake_finetune_table_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_table_state)\n",
    "print(snowflake_finetune_table_state.ragas_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (1st Model) With Semantic Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "Answers created - ready for Ragas evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:12<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation complete\n",
      "{'faithfulness': 0.8889, 'answer_relevancy': 0.9592, 'context_recall': 0.8889, 'context_precision': 1.0000, 'answer_correctness': 0.6295}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snowflake_finetune_semantic_state = ModelRunState()\n",
    "snowflake_finetune_semantic_state.name = \"Snowflake_FineSemantic/1000/100\"\n",
    "snowflake_finetune_semantic_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_semantic_state.qa_model = ChatOpenAI(model=snowflake_finetune_semantic_state.qa_model_name)\n",
    "\n",
    "snowflake_finetune_semantic_state.embedding_model_name = snowflake_finetune_model_name\n",
    "snowflake_finetune_semantic_state.embedding_model = snowflake_finetune_model\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_semantic_state.chunking_strategy = CHUNKING_STRATEGY_SEMANTIC\n",
    "snowflake_finetune_semantic_state.chunk_size = 1000\n",
    "snowflake_finetune_semantic_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_semantic_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_semantic_state)\n",
    "create_answers(app_state, snowflake_finetune_semantic_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_semantic_state)\n",
    "print(snowflake_finetune_semantic_state.ragas_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Fine Tuned Snowflake Model (1st Model) with 3 Different Chunking Strategies\n",
    "\n",
    "Note the Fine Tuned model used simple recursive text with specified chunking size and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Snowflake_Fine/1000/100</th>\n",
       "      <th>Snowflake_FineSection/1000/100</th>\n",
       "      <th>Snowflake_FineTable/1000/100</th>\n",
       "      <th>Snowflake_FineSemantic/1000/100</th>\n",
       "      <th>1v2 Difference</th>\n",
       "      <th>1v3 Difference</th>\n",
       "      <th>1v4 Difference</th>\n",
       "      <th>2v3 Difference</th>\n",
       "      <th>2v4 Difference</th>\n",
       "      <th>3v4 Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.900966</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>-0.218097</td>\n",
       "      <td>-0.021368</td>\n",
       "      <td>-0.208806</td>\n",
       "      <td>-0.012077</td>\n",
       "      <td>0.196729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.945477</td>\n",
       "      <td>0.969683</td>\n",
       "      <td>0.945677</td>\n",
       "      <td>0.959232</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.013755</td>\n",
       "      <td>-0.024006</td>\n",
       "      <td>-0.010451</td>\n",
       "      <td>0.013555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.417840</td>\n",
       "      <td>0.370029</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.629459</td>\n",
       "      <td>-0.047811</td>\n",
       "      <td>0.066959</td>\n",
       "      <td>0.211619</td>\n",
       "      <td>0.114771</td>\n",
       "      <td>0.259430</td>\n",
       "      <td>0.144660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric  Snowflake_Fine/1000/100  \\\n",
       "0        faithfulness                 0.910256   \n",
       "1    answer_relevancy                 0.945477   \n",
       "2      context_recall                 0.888889   \n",
       "3   context_precision                 1.000000   \n",
       "4  answer_correctness                 0.417840   \n",
       "\n",
       "   Snowflake_FineSection/1000/100  Snowflake_FineTable/1000/100  \\\n",
       "0                        0.900966                      0.692160   \n",
       "1                        0.969683                      0.945677   \n",
       "2                        0.888889                      0.888889   \n",
       "3                        1.000000                      1.000000   \n",
       "4                        0.370029                      0.484800   \n",
       "\n",
       "   Snowflake_FineSemantic/1000/100  1v2 Difference  1v3 Difference  \\\n",
       "0                         0.888889       -0.009290       -0.218097   \n",
       "1                         0.959232        0.024206        0.000200   \n",
       "2                         0.888889        0.000000        0.000000   \n",
       "3                         1.000000        0.000000        0.000000   \n",
       "4                         0.629459       -0.047811        0.066959   \n",
       "\n",
       "   1v4 Difference  2v3 Difference  2v4 Difference  3v4 Difference  \n",
       "0       -0.021368       -0.208806       -0.012077        0.196729  \n",
       "1        0.013755       -0.024006       -0.010451        0.013555  \n",
       "2        0.000000        0.000000        0.000000        0.000000  \n",
       "3        0.000000        0.000000        0.000000        0.000000  \n",
       "4        0.211619        0.114771        0.259430        0.144660  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_results_4(run_model_1, run_model_2, run_model_3, run_model_4):\n",
    "    # Extract results for each model\n",
    "    results_1 = run_model_1.ragas_results\n",
    "    results_2 = run_model_2.ragas_results\n",
    "    results_3 = run_model_3.ragas_results\n",
    "    results_4 = run_model_4.ragas_results\n",
    "\n",
    "    # Create comparison data\n",
    "    comparison_data = {\n",
    "        'Metric': list(results_1.keys()),\n",
    "        run_model_1.name: [results_1[key] for key in results_1.keys()],\n",
    "        run_model_2.name: [results_2[key] for key in results_2.keys()],\n",
    "        run_model_3.name: [results_3[key] for key in results_3.keys()],\n",
    "        run_model_4.name: [results_4[key] for key in results_4.keys()],\n",
    "        '1v2 Difference': [results_2[key] - results_1[key] for key in results_1.keys()],\n",
    "        '1v3 Difference': [results_3[key] - results_1[key] for key in results_1.keys()],\n",
    "        '1v4 Difference': [results_4[key] - results_1[key] for key in results_1.keys()],\n",
    "        '2v3 Difference': [results_3[key] - results_2[key] for key in results_2.keys()],\n",
    "        '2v4 Difference': [results_4[key] - results_2[key] for key in results_2.keys()],\n",
    "        '3v4 Difference': [results_4[key] - results_3[key] for key in results_3.keys()]\n",
    "    }\n",
    "\n",
    "    # Return the dataframe\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "df = compare_results_4(snowflake_finetune_state , snowflake_finetune_section_state, snowflake_finetune_table_state, snowflake_finetune_semantic_state)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Snowflake Model (2nd Model Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "snowflake_finetune_2_state = ModelRunState()\n",
    "snowflake_finetune_2_state.name = \"Snowflake_Fine_2/1000/100\"\n",
    "snowflake_finetune_2_state.qa_model_name = \"gpt-4o-mini\"\n",
    "snowflake_finetune_2_state.qa_model = ChatOpenAI(model=snowflake_finetune_2_state.qa_model_name)\n",
    "\n",
    "# finetune snowflake embedding model\n",
    "\n",
    "hf_username = \"rchrdgwr\"\n",
    "hf_repo_name = \"finetuned-arctic-model-2\"\n",
    "\n",
    "# Load the fine-tuned model from Hugging Face\n",
    "snowflake_finetune_2_state.embedding_model_name = f\"{hf_username}/{hf_repo_name}\"\n",
    "snowflake_finetune_2_state.embedding_model = HuggingFaceEmbeddings(model_name=snowflake_finetune_2_state.embedding_model_name)\n",
    "\n",
    "# use same chunk size as before\n",
    "snowflake_finetune_2_state.chunk_size = 1000\n",
    "snowflake_finetune_2_state.chunk_overlap = 100\n",
    "create_vector_store(app_state, snowflake_finetune_2_state)\n",
    "\n",
    "create_rag_chain(app_state, snowflake_finetune_2_state)\n",
    "create_answers(app_state, snowflake_finetune_2_state, ragas_state)\n",
    "run_ragas_evaluation(app_state, snowflake_finetune_2_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Two Fine Tuned Snowflake Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = compare_results(snowflake_finetune_state, snowflake_finetune_2_state )\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
